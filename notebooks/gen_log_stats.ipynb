{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8970a0b7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743789d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpic2012 = {\n",
    "    \"test\": \"C://Users//nikol//MT-repo//data//bpic2012_a//bpic2012_a_TEST.csv\",\n",
    "        \"gen_cvae\": \"C://Users//nikol//MT-repo//logs//eval//runs//2025-10-03_17-54-49//baseline_cvae_rerun_test//generated\", \n",
    "        \"gen_flow\": \"C://Users//nikol//MT-repo//logs//eval//runs//2025-10-15_14-03-31//baseline_cvae_rerun_test//generated\",\n",
    "        \"gen_flow_beam\": \"C://Users//nikol//MT-repo//logs//eval//runs//2025-10-15_19-20-58//baseline_cvae_rerun_test//generated\"\n",
    "        }\n",
    "\n",
    "sepsis = {\n",
    "    \"test\": \"C://Users//nikol//MT-repo//data//sepsis//sepsis_TEST.csv\",\n",
    "    \"gen_cvae\" : \"C://Users//nikol//MT-repo//logs//train//runs//2025-09-28_18-52-04//baseline_cvae_rerun_test//generated\",\n",
    "    \"gen_flow\": \"C://Users//nikol//MT-repo//logs//eval//runs//2025-09-25_10-48-59//baseline_cvae_rerun_test//generated\",\n",
    "    \"gen_flow_beam\": \"C://Users//nikol//MT-repo//logs//train//runs//2025-09-26_20-07-38//baseline_cvae_rerun_test//generated\"\n",
    "}\n",
    "\n",
    "emergency = {\n",
    "    \"test\": \"C://Users//nikol//MT-repo//data//emergency_ORT//emergency_ORT_TEST.csv\",\n",
    "    \"gen_cvae\": \"C://Users//nikol//MT-repo//logs//train//runs//2025-11-15_21-10-43//baseline_cvae_rerun_test//generated\",\n",
    "    \"gen_flow\": \"C://Users//nikol//MT-repo//logs//eval//runs//2025-11-15_17-32-17//baseline_cvae_rerun_test//generated\",\n",
    "    \"gen_flow_beam\": \"C://Users//nikol//MT-repo//logs//train//runs//2025-11-14_21-18-42//baseline_cvae_rerun_test//generated\"\n",
    "}\n",
    "\n",
    "dataset_name = 'bpic2012'  # Options: 'bpic2012', 'sepsis', 'emergency'\n",
    "\n",
    "data = bpic2012  # Options: bpic2012, sepsis, emergency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helpers ----------\n",
    "\n",
    "def _unify_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if {'case:concept:name', 'concept:name', 'org:resource'}.issubset(df.columns):\n",
    "        pass\n",
    "    elif {'Case ID', 'Activity', 'Resource'}.issubset(df.columns):\n",
    "        df = df.rename(columns={\n",
    "            'Case ID': 'case:concept:name',\n",
    "            'Activity': 'concept:name',\n",
    "            'Resource': 'org:resource',\n",
    "        })\n",
    "    else:\n",
    "        raise ValueError(\"Missing required columns: expected either ['case:concept:name','concept:name','org:resource'] or ['Case ID','Activity','Resource']\")\n",
    "    if 'time:timestamp' in df.columns:\n",
    "        df['time:timestamp'] = pd.to_datetime(df['time:timestamp'], errors='coerce')\n",
    "    else:\n",
    "        df['time:timestamp'] = pd.NaT\n",
    "    return df\n",
    "\n",
    "# Normalize resources similar to log_stats: strip and lower; treat only truly empty tokens as placeholders\n",
    "# Relaxed placeholder set to avoid dropping legitimate labels like '-' or 'unknown' in some datasets\n",
    "_RESOURCE_PLACEHOLDERS = {'', 'nan', 'nat', 'none', 'null', 'na', 'n/a'}\n",
    "\n",
    "def _normalize_resources_series(s: pd.Series) -> pd.Series:\n",
    "    if s is None:\n",
    "        return s\n",
    "    s2 = s.astype(str).str.strip()\n",
    "    s2_lower = s2.str.lower()\n",
    "    s2_clean = s2_lower.mask(s2_lower.isin(_RESOURCE_PLACEHOLDERS))\n",
    "    return s2_clean\n",
    "\n",
    "\n",
    "def event_log_stats(df: pd.DataFrame, dataset_name: str = \"log\") -> pd.DataFrame:\n",
    "    df = _unify_schema(df)\n",
    "    n_cases = df['case:concept:name'].nunique()\n",
    "    n_events = len(df)\n",
    "    n_activities = df['concept:name'].nunique()\n",
    "    # Resource normalization\n",
    "    if 'org:resource' in df.columns:\n",
    "        res_norm = _normalize_resources_series(df['org:resource'])\n",
    "        n_resources = int(res_norm.dropna().nunique())\n",
    "        df['__resource_norm__'] = res_norm\n",
    "    else:\n",
    "        n_resources = 0\n",
    "    df_sorted = df.sort_values(['case:concept:name', 'time:timestamp'])\n",
    "    case_sizes = df_sorted.groupby('case:concept:name').size() if n_cases > 0 else pd.Series(dtype=int)\n",
    "    if df['time:timestamp'].notna().any() and n_cases > 0:\n",
    "        first_last = df_sorted.groupby('case:concept:name')['time:timestamp'].agg(['min', 'max'])\n",
    "        tpt = (first_last['max'] - first_last['min']).dt.total_seconds() / 60.0\n",
    "    else:\n",
    "        tpt = pd.Series(dtype=float)\n",
    "    window_start = df['time:timestamp'].min()\n",
    "    window_end = df['time:timestamp'].max()\n",
    "    total_duration_hours = None\n",
    "    if pd.notna(window_start) and pd.notna(window_end):\n",
    "        total_duration_hours = float((window_end - window_start).total_seconds() / 3600.0)\n",
    "    row = dict(\n",
    "        dataset=dataset_name,\n",
    "        n_cases=int(n_cases),\n",
    "        n_variants=int(df_sorted.groupby('case:concept:name')['concept:name'].apply(tuple).nunique() if n_cases > 0 else 0),\n",
    "        n_activities=int(n_activities),\n",
    "        n_resources=int(n_resources),\n",
    "        trace_len_mean=float(case_sizes.mean()) if case_sizes.size else 0.0,\n",
    "        trace_len_max=int(case_sizes.max()) if case_sizes.size else 0,\n",
    "        tpt_mean=float(tpt.mean()) if tpt.size else None,  # minutes\n",
    "        total_duration_hours=total_duration_hours,\n",
    "    )\n",
    "    return pd.DataFrame([row])\n",
    "\n",
    "\n",
    "def _read_test_csv(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Test CSV not found: {path}\")\n",
    "    for sep in [';', ',']:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=sep)\n",
    "            return _unify_schema(df)\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(f\"Failed to read test CSV with delimiters ';' or ',' : {path}\")\n",
    "\n",
    "\n",
    "def _load_train_from_any_model(data_dict: dict):\n",
    "    for key, folder in data_dict.items():\n",
    "        if key == 'test':\n",
    "            continue\n",
    "        files = sorted(glob.glob(os.path.join(folder, 'train_SPLIT_*.csv')))\n",
    "        if not files:\n",
    "            continue\n",
    "        dfs = []\n",
    "        for f in files:\n",
    "            try:\n",
    "                df = pd.read_csv(f, sep=';')\n",
    "                dfs.append(_unify_schema(df))\n",
    "            except Exception:\n",
    "                continue\n",
    "        if dfs:\n",
    "            out = pd.concat(dfs, ignore_index=True)\n",
    "            print(f\"TRAIN loader: {len(files)} train splits -> cases: {out['case:concept:name'].nunique()} events: {len(out)}\")\n",
    "            return out\n",
    "    return pd.DataFrame(columns=['case:concept:name','concept:name','org:resource','time:timestamp'])\n",
    "\n",
    "\n",
    "def aggregate_generated_model(folder: str, pattern: str = 'gen*.csv', dataset_name: str = 'GEN'):\n",
    "    files = sorted(glob.glob(os.path.join(folder, pattern)))\n",
    "    if not files:\n",
    "        files = sorted(glob.glob(os.path.join(folder, '*.csv')))\n",
    "    rows = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_csv(f, sep=';')\n",
    "            stats = event_log_stats(df, dataset_name=os.path.splitext(os.path.basename(f))[0])\n",
    "            rows.append(stats.iloc[0].to_dict())\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not rows:\n",
    "        return None\n",
    "    detail_df = pd.DataFrame(rows)\n",
    "    # Aggregate hours as mean of per-file total duration (fallback 0 if all missing)\n",
    "    hours_series = detail_df['total_duration_hours'].dropna()\n",
    "    total_hours = float(hours_series.mean()) if len(hours_series) else 0.0\n",
    "    avg_row = {\n",
    "        'dataset': dataset_name,\n",
    "        'n_cases': float(detail_df['n_cases'].mean()),\n",
    "        'n_variants': float(detail_df['n_variants'].mean()),\n",
    "        'n_activities': float(detail_df['n_activities'].mean()),\n",
    "        'n_resources': float(detail_df['n_resources'].mean()),\n",
    "        'trace_len_mean': float(detail_df['trace_len_mean'].mean()),\n",
    "        'trace_len_max': float(detail_df['trace_len_max'].max()),\n",
    "        'tpt_mean': float(detail_df['tpt_mean'].mean()) if 'tpt_mean' in detail_df and detail_df['tpt_mean'].notna().any() else None,\n",
    "        'total_duration_hours': total_hours,\n",
    "    }\n",
    "    #print(f\"Generated aggregation: folder={folder} files={len(detail_df)} avg_cases={avg_row['n_cases']:.1f}\")\n",
    "    return avg_row\n",
    "\n",
    "# ---------- Build rows ----------\n",
    "rows = []\n",
    "if 'test' in data:\n",
    "    test_df = _read_test_csv(data['test'])\n",
    "    rows.append(event_log_stats(test_df, 'TEST').iloc[0].to_dict())\n",
    "train_df = _load_train_from_any_model(data)\n",
    "if len(train_df):\n",
    "    rows.append(event_log_stats(train_df, 'TRAIN').iloc[0].to_dict())\n",
    "for key, folder in data.items():\n",
    "    if key == 'test':\n",
    "        continue\n",
    "    agg = aggregate_generated_model(folder, dataset_name=key)\n",
    "    if agg:\n",
    "        rows.append(agg)\n",
    "\n",
    "summary_source_df = pd.DataFrame(rows)\n",
    "\n",
    "# ---------- Presentation summary ----------\n",
    "summary_rows = []\n",
    "for _, r in summary_source_df.iterrows():\n",
    "    mean_len = r.get('trace_len_mean')\n",
    "    tpt_min = r.get('tpt_mean')\n",
    "    avg_tpt_h = float(tpt_min) / 60.0 if pd.notna(tpt_min) else None\n",
    "    avg_tpt_days = avg_tpt_h / 24.0 if avg_tpt_h is not None else None\n",
    "    total_h = r.get('total_duration_hours', 0.0) or 0.0  # ensure not None/NaN\n",
    "    total_days = total_h / 24.0\n",
    "    total_years = total_h / (24.0 * 365.0)\n",
    "    summary_rows.append({\n",
    "        'Dataset': r['dataset'],\n",
    "        'Traces': int(round(r['n_cases'])) if pd.notna(r['n_cases']) else None,\n",
    "        'Variants': int(round(r['n_variants'])) if pd.notna(r['n_variants']) else None,\n",
    "        'Activities': int(round(r['n_activities'])) if pd.notna(r['n_activities']) else None,\n",
    "        'Resources': int(round(r['n_resources'])) if pd.notna(r['n_resources']) else None,\n",
    "        'Avg. trace length': f\"{mean_len:.2f}\" if mean_len is not None else None,\n",
    "        'Max trace length': int(round(r['trace_len_max'])) if pd.notna(r['trace_len_max']) else None,\n",
    "        'Avg. trace cycle time (h)': round(avg_tpt_h, 2) if avg_tpt_h is not None else None,\n",
    "        'Avg. trace cycle time (days)': round(avg_tpt_days, 2) if avg_tpt_days is not None else None,\n",
    "        'Total duration (h)': round(total_h, 2),\n",
    "        'Total duration (days)': round(total_days, 2),\n",
    "        'Total duration (years)': round(total_years, 4),\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows, columns=[\n",
    "    'Dataset','Traces','Variants','Activities','Resources','Avg. trace length','Max trace length','Avg. trace cycle time (h)','Avg. trace cycle time (days)','Total duration (h)','Total duration (days)','Total duration (years)'\n",
    "])\n",
    "\n",
    "# Save summary only into nested folder \"gen logs stats\"\n",
    "out_dir = os.path.abspath(os.path.join('..', 'output')) if os.path.basename(os.getcwd()) == 'notebooks' else os.path.join('output')\n",
    "stats_dir = os.path.join(out_dir, 'gen logs stats')\n",
    "os.makedirs(stats_dir, exist_ok=True)\n",
    "summary_path = os.path.join(stats_dir, f'gen_log_stats_{dataset_name}.csv')\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"Saved summary stats to: {summary_path}\")\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helpers ----------\n",
    "def _read_csv_any(path: str) -> pd.DataFrame:\n",
    "    for sep in [';', ',']:\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=sep)\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(f\"Failed to read CSV with ';' or ',' : {path}\")\n",
    "\n",
    "\n",
    "def _collect_activity_counts_from_folder(folder: str, pattern: str = 'gen*.csv') -> pd.Series:\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"Generated folder missing: {folder}\")\n",
    "        return pd.Series(dtype=int)\n",
    "    files = sorted(glob.glob(os.path.join(folder, pattern)))\n",
    "    if not files:\n",
    "        files = sorted(glob.glob(os.path.join(folder, '*.csv')))\n",
    "    counts = {}\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = _read_csv_any(f)\n",
    "            # Reuse schema normalizer from previous cell if available\n",
    "            if '_unify_schema' in globals():\n",
    "                df = _unify_schema(df)\n",
    "            if 'concept:name' in df.columns:\n",
    "                vc = df['concept:name'].value_counts()\n",
    "                for k, v in vc.items():\n",
    "                    counts[k] = counts.get(k, 0) + int(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.Series(counts, dtype=int).sort_index()\n",
    "\n",
    "\n",
    "# ---------- Build counts ----------\n",
    "# TEST (will be dropped from plot later if present)\n",
    "test_df = _read_test_csv(data['test']) if 'test' in data else pd.DataFrame()\n",
    "if len(test_df) > 0 and '_unify_schema' in globals():\n",
    "    test_df = _unify_schema(test_df)\n",
    "\n",
    "counts = {}\n",
    "if len(test_df) > 0:\n",
    "    counts['TEST'] = test_df['concept:name'].value_counts().sort_index()\n",
    "\n",
    "# TRAIN (use already loaded train_df if available, otherwise attempt to load)\n",
    "_train_df_candidate = None\n",
    "try:\n",
    "    if 'train_df' in globals() and isinstance(train_df, pd.DataFrame) and len(train_df) > 0:\n",
    "        _train_df_candidate = train_df\n",
    "    elif '_load_train_from_any_model' in globals():\n",
    "        _train_df_candidate = _load_train_from_any_model(data)\n",
    "except Exception:\n",
    "    _train_df_candidate = None\n",
    "\n",
    "if _train_df_candidate is None or len(_train_df_candidate) == 0:\n",
    "    raise RuntimeError(\"No TRAIN data available; cannot build train-based comparison plot.\")\n",
    "if '_unify_schema' in globals():\n",
    "    _train_df_candidate = _unify_schema(_train_df_candidate)\n",
    "counts['TRAIN'] = _train_df_candidate['concept:name'].value_counts().sort_index()\n",
    "\n",
    "# Generated models if present in the selected dataset dict\n",
    "for key in ['gen_cvae', 'gen_flow', 'gen_flow_beam']:\n",
    "    if key in data:\n",
    "        counts[key] = _collect_activity_counts_from_folder(data[key])\n",
    "\n",
    "# Combine into one DataFrame with union of activities\n",
    "all_index = pd.Index([])\n",
    "for s in counts.values():\n",
    "    all_index = all_index.union(s.index)\n",
    "\n",
    "comb = pd.DataFrame(index=all_index)\n",
    "for name, s in counts.items():\n",
    "    comb[name] = s\n",
    "comb = comb.fillna(0).astype(int)\n",
    "\n",
    "# Drop TEST; keep TRAIN + generated only\n",
    "comb = comb.drop(columns=['TEST'], errors='ignore')\n",
    "\n",
    "# Sort/limit uses TRAIN as reference now\n",
    "TOP_N = 60  # adjust to see more/less\n",
    "\n",
    "# Report activities not generated per model (zeros) for generated datasets only\n",
    "for col in [c for c in comb.columns if c != 'TRAIN']:\n",
    "    missing = comb.index[comb[col] == 0]\n",
    "    if len(missing):\n",
    "        print(f\"Activities NOT generated by {col}: {len(missing)}\")\n",
    "        # Print a short preview to avoid overwhelming output\n",
    "        print(list(missing[:20]))\n",
    "\n",
    "# ---------- Convert to percentages (% of all events per dataset) ----------\n",
    "col_sums = comb.sum(axis=0)\n",
    "col_sums_safe = col_sums.replace(0, np.nan)\n",
    "comb_pct = comb.divide(col_sums_safe, axis=1) * 100.0\n",
    "\n",
    "# Sort by TRAIN percentage and limit to TOP_N\n",
    "sort_ref = 'TRAIN' if 'TRAIN' in comb_pct.columns else comb_pct.columns[0]\n",
    "comb_sorted = comb_pct.sort_values(by=sort_ref, ascending=False)\n",
    "if TOP_N is not None:\n",
    "    comb_sorted = comb_sorted.head(TOP_N)\n",
    "\n",
    "# ---------- Plot: Horizontal grouped bars (percentages) with min visible floor ----------\n",
    "MIN_VISIBLE_PCT = 0.05  # show tiny non-zero shares at least as 0.05% so bars are visible\n",
    "long_df = comb_sorted.reset_index().melt(id_vars='index', var_name='Dataset', value_name='Percent')\n",
    "long_df = long_df.rename(columns={'index': 'Activity'})\n",
    "# Ensure consistent Dataset order\n",
    "cat_order = [c for c in ['TRAIN', 'gen_cvae', 'gen_flow', 'gen_flow_beam'] if c in long_df['Dataset'].unique()]\n",
    "long_df['Dataset'] = pd.Categorical(long_df['Dataset'], categories=cat_order, ordered=True)\n",
    "# Floor tiny non-zero values for plotting visibility (keep 0 exactly 0)\n",
    "long_df['PlotPercent'] = np.where((long_df['Percent'] > 0) & (long_df['Percent'] < MIN_VISIBLE_PCT), MIN_VISIBLE_PCT, long_df['Percent'])\n",
    "\n",
    "height = max(6, 0.35 * len(comb_sorted))\n",
    "plt.figure(figsize=(14, height))\n",
    "palette = {\n",
    "    'TRAIN': \"#ff7f0e\",\n",
    "    'gen_cvae': '#1f77b4',\n",
    "    'gen_flow': '#2ca02c',\n",
    "    'gen_flow_beam': '#9467bd'\n",
    "}\n",
    "ax = sns.barplot(\n",
    "    data=long_df,\n",
    "    y='Activity', x='PlotPercent', hue='Dataset', orient='h',\n",
    "    palette=[palette.get(k, None) for k in cat_order]\n",
    ")\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "ax.set_title(f\"Activity share â€” {dataset_name.upper()}\\n(% of events per dataset)\")\n",
    "ax.set_xlabel('Percent of events (%) ')\n",
    "ax.set_ylabel('Activity')\n",
    "# Remap legend labels to friendly names\n",
    "legend_map = {\n",
    "    'TRAIN': 'Train',\n",
    "    'gen_cvae': 'CVAE',\n",
    "    'gen_flow': 'TF-decoder',\n",
    "    'gen_flow_beam': 'TF-decoder (beam)'\n",
    "}\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, [legend_map.get(l, l) for l in labels], title='Dataset', loc='lower right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure only in dataset-specific folder\n",
    "out_root = os.path.abspath(os.path.join('..', 'output')) if os.path.basename(os.getcwd()) == 'notebooks' else os.path.join('output')\n",
    "dataset_dir = os.path.join(out_root, dataset_name.lower())\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "fig_path = os.path.join(dataset_dir, f'activity_frequency_{dataset_name}_hbar_pct.pdf')\n",
    "plt.savefig(fig_path, format='pdf', bbox_inches='tight')\n",
    "\n",
    "print(f\"Saved activity frequency comparison (percent) to: {fig_path}\")\n",
    "\n",
    "# Show table as cell output (percentages)\n",
    "comb_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
