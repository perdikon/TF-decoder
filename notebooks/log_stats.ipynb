{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae19af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpic_path = \"C://Users//nikol//MT-repo//data//bpic2012_a//bpic2012_a.csv\"\n",
    "sepsis_path = \"C://Users//nikol//MT-repo//data//sepsis//sepsis.csv\"\n",
    "emergency_ort_path = \"C://Users//nikol//MT-repo//data//emergency_ORT/emergency_ORT.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d142a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities to load logs and compute statistics\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List\n",
    "\n",
    "# Diagnostics toggles\n",
    "VERBOSE = False\n",
    "DIAGNOSE_RESOURCES = False\n",
    "DIAG_PRINT_ALL_RESOURCES = False\n",
    "DIAG_RESOURCE_MAX_SHOW = 500\n",
    "\n",
    "# Heuristics to detect common column names across different logs\n",
    "CASE_CANDIDATES = [\n",
    "    'Case ID', 'case_id', 'caseid', 'case', 'case:concept:name', 'SEHRegistratienummer'\n",
    "]\n",
    "ACTIVITY_CANDIDATES = [\n",
    "    'activity', 'Activity', 'concept:name', 'event', 'event_name'\n",
    "]\n",
    "TIMESTAMP_CANDIDATES = [\n",
    "    'timestamp', 'Timestamp', 'time:timestamp', 'time', 'event_time'\n",
    "]\n",
    "RESOURCE_CANDIDATES = [\n",
    "    'resource', 'Resource', 'org:resource', 'performer'\n",
    "]\n",
    "\n",
    "\n",
    "def pick_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for name in candidates:\n",
    "        if name in df.columns:\n",
    "            return name\n",
    "        # case-insensitive match\n",
    "        if name.lower() in cols_lower:\n",
    "            return cols_lower[name.lower()]\n",
    "    return None\n",
    "\n",
    "\n",
    "def ensure_datetime(s: pd.Series) -> pd.Series:\n",
    "    if pd.api.types.is_datetime64_any_dtype(s):\n",
    "        return s\n",
    "    return pd.to_datetime(s, errors='coerce', utc=False, infer_datetime_format=True)\n",
    "\n",
    "\n",
    "def compute_log_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    case_col = pick_column(df, CASE_CANDIDATES)\n",
    "    act_col = pick_column(df, ACTIVITY_CANDIDATES)\n",
    "    ts_col = pick_column(df, TIMESTAMP_CANDIDATES)\n",
    "    res_col = pick_column(df, RESOURCE_CANDIDATES)\n",
    "\n",
    "    if case_col is None or act_col is None or ts_col is None:\n",
    "        missing = [n for n, v in [('case', case_col), ('activity', act_col), ('timestamp', ts_col)] if v is None]\n",
    "        raise ValueError(f\"Missing required column(s): {', '.join(missing)}\")\n",
    "\n",
    "    # Minimal cleaning\n",
    "    df = df[[c for c in [case_col, act_col, ts_col, res_col] if c is not None]].copy()\n",
    "    df[act_col] = df[act_col].astype(str).str.strip()\n",
    "    df[ts_col] = ensure_datetime(df[ts_col])\n",
    "    df = df.dropna(subset=[case_col, act_col, ts_col])\n",
    "\n",
    "    # Resource normalization to avoid overcounting due to whitespace/case/placeholder tokens\n",
    "    resources_raw_unique = 0\n",
    "    resources_clean_unique = 0\n",
    "    placeholder_counts: Dict[str, int] = {}\n",
    "    if res_col is not None and res_col in df.columns:\n",
    "        # Raw, as-is (excluding real NaNs)\n",
    "        res_raw = df[res_col]\n",
    "        resources_raw_unique = int(res_raw.dropna().nunique())\n",
    "\n",
    "        # Clean: strip, case-fold, and nullify placeholder tokens\n",
    "        res_s = res_raw.astype(str).str.strip()\n",
    "        res_lower = res_s.str.lower()\n",
    "        # Tokens that should not be treated as real resources\n",
    "        empty_like = {'', 'nan', 'nat', 'none', 'null', 'na', 'n/a', '-', 'unknown', 'unk'}\n",
    "        mask_placeholder = res_lower.isin(empty_like)\n",
    "        # Track placeholder counts for diagnostics\n",
    "        if DIAGNOSE_RESOURCES:\n",
    "            for tok in sorted(empty_like):\n",
    "                placeholder_counts[tok] = int((res_lower == tok).sum())\n",
    "        # Apply cleaning: set placeholders to NaN, but keep original case (we count on lower values below)\n",
    "        res_clean = res_s.mask(mask_placeholder)\n",
    "        res_norm = res_clean.str.lower()\n",
    "        resources_clean_unique = int(res_norm.dropna().nunique())\n",
    "\n",
    "        # Optionally attach a normalized column for downstream debugging (not used in summary)\n",
    "        df['__resource_norm__'] = res_norm\n",
    "\n",
    "    # Sort to build consistent variants\n",
    "    df = df.sort_values([case_col, ts_col, act_col]).reset_index(drop=True)\n",
    "\n",
    "    # Core counts\n",
    "    traces = int(df[case_col].nunique())\n",
    "    activities = int(df[act_col].nunique())\n",
    "    # Use cleaned resource count exclusively; if no resource column, use 0\n",
    "    resources = int(resources_clean_unique) if (res_col is not None and res_col in df.columns) else 0\n",
    "\n",
    "    # Per-case metrics\n",
    "    events_per_case = df.groupby(case_col)[act_col].size()\n",
    "    avg_trace_length = float(events_per_case.mean()) if not events_per_case.empty else 0.0\n",
    "    max_trace_length = int(events_per_case.max()) if not events_per_case.empty else 0\n",
    "\n",
    "    # Variants (sequence of activities per case)\n",
    "    seq_per_case = df.groupby(case_col)[act_col].apply(lambda s: ' > '.join(s.tolist()))\n",
    "    variants = int(seq_per_case.nunique())\n",
    "\n",
    "    # Cycle times per case (max - min)\n",
    "    ts_agg = df.groupby(case_col)[ts_col].agg(['min', 'max'])\n",
    "    cycle_times = (ts_agg['max'] - ts_agg['min']).dt.total_seconds().div(3600.0)  # hours\n",
    "    avg_cycle_time_h = float(cycle_times.mean()) if not cycle_times.empty else 0.0\n",
    "\n",
    "    # Total duration over the whole log\n",
    "    total_duration_h = float((df[ts_col].max() - df[ts_col].min()).total_seconds() / 3600.0) if len(df) else 0.0\n",
    "\n",
    "    # Diagnostics output to help understand over-counting\n",
    "    if DIAGNOSE_RESOURCES and (res_col is not None and res_col in df.columns):\n",
    "        print(\"\\n[Resource diagnostics]\")\n",
    "        print(f\"- Raw unique resources (non-null): {resources_raw_unique}\")\n",
    "        print(f\"- Cleaned unique resources: {resources_clean_unique}\")\n",
    "        if placeholder_counts:\n",
    "            nonzero = {k: v for k, v in placeholder_counts.items() if v > 0}\n",
    "            if nonzero:\n",
    "                print(\"- Placeholder token occurrences (treated as empty):\", nonzero)\n",
    "        # Show top 20 resource values by frequency (cleaned)\n",
    "        vc = df['__resource_norm__'].value_counts(dropna=True)\n",
    "        if not vc.empty:\n",
    "            print(\"- Top resource values (cleaned, top 20):\")\n",
    "            display(vc.head(20).rename_axis('resource').reset_index(name='events'))\n",
    "        # Optionally print all unique cleaned resources\n",
    "        if DIAG_PRINT_ALL_RESOURCES:\n",
    "            uniques = df['__resource_norm__'].dropna().drop_duplicates().sort_values()\n",
    "            to_show = uniques.head(DIAG_RESOURCE_MAX_SHOW)\n",
    "            print(f\"- Unique cleaned resources (showing up to {DIAG_RESOURCE_MAX_SHOW}): {len(uniques)} total\")\n",
    "            display(pd.DataFrame({'resource': to_show}))\n",
    "\n",
    "    summary = pd.DataFrame([\n",
    "        {\n",
    "            'Traces': traces,\n",
    "            'Variants': variants,\n",
    "            'Activities': activities,\n",
    "            # Cleaned/preferred resources only\n",
    "            'Resources': resources,\n",
    "            'Avg. trace length': round(avg_trace_length, 2),\n",
    "            'Max trace length': int(max_trace_length),\n",
    "            'Avg. trace cycle time (h)': round(avg_cycle_time_h, 2),\n",
    "            'Total duration (h)': round(total_duration_h, 2),\n",
    "        }\n",
    "    ])\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "def try_load_csv(path_str: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        p = Path(path_str)\n",
    "        if not p.exists():\n",
    "            if VERBOSE:\n",
    "                print(f\"Not found: {p}\")\n",
    "            return None\n",
    "        df = pd.read_csv(p, delimiter=';')\n",
    "        if VERBOSE:\n",
    "            print(f\"Loaded {p} -> {len(df):,} rows, {len(df.columns)} cols\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        if VERBOSE:\n",
    "            print(f\"Failed to load {path_str}: {type(e).__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Compute and display stats for known paths\n",
    "paths = {\n",
    "    'BPIC2012_A': bpic_path,\n",
    "    'Sepsis': sepsis_path,\n",
    "    # Attempt to include emergency ED log if present\n",
    "    'Emergency_ort': emergency_ort_path,\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, p in paths.items():\n",
    "    df = try_load_csv(p)\n",
    "    if df is None or df.empty:\n",
    "        continue\n",
    "    try:\n",
    "        if VERBOSE:\n",
    "            print(f\"\\n=== Dataset: {name} ===\")\n",
    "        stats = compute_log_stats(df)\n",
    "        stats.insert(0, 'Dataset', name)\n",
    "        rows.append(stats)\n",
    "    except Exception as e:\n",
    "        if VERBOSE:\n",
    "            print(f\"Skipping {name}: {type(e).__name__}: {e}\")\n",
    "\n",
    "if rows:\n",
    "    all_stats = pd.concat(rows, ignore_index=True)\n",
    "    # Only print the full combined table\n",
    "    display(all_stats)\n",
    "else:\n",
    "    print(\"No datasets loaded; please check the paths.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
