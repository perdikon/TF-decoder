{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ea21aa",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97019f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import unicodedata\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Optional: display options\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "pd.set_option('display.width', 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bed25",
   "metadata": {},
   "source": [
    "# Read CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d788070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small robust CSV loader: try strict first, then fall back to tolerant read\n",
    "def load_csv_robust(csv_path: Path, sep: str = ';') -> pd.DataFrame:\n",
    "    print(f\"Using: {csv_path}\")\n",
    "    try:\n",
    "        df_local = pd.read_csv(csv_path, sep=sep)\n",
    "        used_engine = 'c'\n",
    "    except Exception as e:\n",
    "        print(f\"Strict read failed: {type(e).__name__}: {e}\")\n",
    "        df_local = pd.read_csv(csv_path, sep=sep, engine='python', on_bad_lines='skip')\n",
    "        used_engine = 'python'\n",
    "    print(f\"Loaded {len(df_local):,} rows, {len(df_local.columns)} columns (engine={used_engine})\")\n",
    "    return df_local\n",
    "\n",
    "# File paths (absolute to avoid working-directory issues)\n",
    "base_root = Path(r\"C:\\Users\\nikol\\MT-repo\")\n",
    "base_log_path = base_root / 'data/201026 SEH Basis en Triage.csv'\n",
    "orders_path = base_root / 'data/201026 SEH Orders.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4092d9",
   "metadata": {},
   "source": [
    "### Base Log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the main ED registrations/triage CSV\n",
    "df = load_csv_robust(base_log_path, sep=';')\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b5106",
   "metadata": {},
   "source": [
    "### Orders log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Orders CSV\n",
    "orders_df = load_csv_robust(orders_path, sep=';')\n",
    "display(orders_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542e772",
   "metadata": {},
   "source": [
    "### Filter by Specialismecode (right after import)\n",
    "Reduce the dataset early by keeping only rows whose `Specialismecode` is in a provided list.\n",
    "- Applies to both the base log (`df`) and the Orders log (`orders_df`).\n",
    "- Set the list of codes in `SPECIALISMECODES` and run this cell.\n",
    "- If the list is empty, no filtering is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Specialismecodes to keep (exact matches, case-insensitive)\n",
    "# Example: SPECIALISMECODES = ['SEH', 'CARD', 'INT']\n",
    "SPECIALISMECODES = [\"ORT\"]\n",
    "\n",
    "# Normalize to strings for matching\n",
    "_keep = [str(x) for x in SPECIALISMECODES]\n",
    "\n",
    "# Base log filter\n",
    "_before_rows = len(df)\n",
    "if _keep:\n",
    "    df = df[df['Specialismecode'].astype(str).str.upper().isin([c.upper() for c in _keep])].copy()\n",
    "_after_rows = len(df)\n",
    "print(f\"[base] Specialismecode filter: rows {_after_rows}/{_before_rows} kept\" + (\" (no filter list provided)\" if not _keep else \"\"))\n",
    "\n",
    "# Orders filter\n",
    "_before_rows_o = len(orders_df)\n",
    "if _keep:\n",
    "    orders_df = orders_df[orders_df['Specialismecode'].astype(str).str.upper().isin([c.upper() for c in _keep])].copy()\n",
    "_after_rows_o = len(orders_df)\n",
    "print(f\"[orders] Specialismecode filter: rows {_after_rows_o}/{_before_rows_o} kept\" + (\" (no filter list provided)\" if not _keep else \"\"))\n",
    "\n",
    "# Quick peek after filtering\n",
    "display(df.head(3))\n",
    "display(orders_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466fa62a",
   "metadata": {},
   "source": [
    "# Building event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid cases with Age == 0 (if column exists)\n",
    "if 'Age' in df.columns:\n",
    "    age_num = pd.to_numeric(df['Age'], errors='coerce')\n",
    "    mask_zero = age_num == 0\n",
    "    n_drop = int(mask_zero.sum())\n",
    "    if n_drop > 0:\n",
    "        df = df.loc[~mask_zero].copy()\n",
    "    print(f\"Dropped {n_drop} row(s) with Age == 0.\")\n",
    "else:\n",
    "    print(\"Column 'Age' not found; no rows dropped for Age == 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8bebce",
   "metadata": {},
   "source": [
    "### Filter case attributes\n",
    "- Keep only a small, focused set of case-level attributes plus the event date/time columns needed for parsing.\n",
    "- Edit the `keep_case_attrs` list below to include the case attributes you want to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose and keep a focused subset of case attributes\n",
    "case_id_col = \"SEHRegistratienummer\"\n",
    "\n",
    "# Event columns needed later to construct per-activity timestamps\n",
    "raw_event_cols = [\n",
    "    \"RegitrationDate\",\"RegitrationTime\",\n",
    "    \"ArrivelDate\",\"ArrivelTime\",\n",
    "    \"TriageDate\",\"TriageTime\",\n",
    "    \"Start Treatment Date\",\"Start TreatmentTime\",\n",
    "    \"DepartDate\",\"DepartTime\",\n",
    "]\n",
    "\n",
    "# NOTE: Edit this list to include the case attributes you'd like to retain (if present)\n",
    "keep_case_attrs = [\n",
    "    case_id_col,\n",
    "    \"Age\",\n",
    "    \"Gender\",\n",
    "    \"Specialismecode\",\n",
    "]\n",
    "\n",
    "# Compute which columns are present and keep them, alongside event columns\n",
    "present_attrs = [c for c in keep_case_attrs if c in df.columns]\n",
    "present_event_cols = [c for c in raw_event_cols if c in df.columns]\n",
    "cols_to_keep = list(dict.fromkeys([case_id_col] + present_attrs + present_event_cols))  # preserve order, unique\n",
    "\n",
    "_before_cols = len(df.columns)\n",
    "df = df.loc[:, [c for c in cols_to_keep if c in df.columns]].copy()\n",
    "\n",
    "print(f\"Filtered columns: kept {len(df.columns)} of {_before_cols}.\")\n",
    "print(\"Kept columns:\", df.columns.tolist())\n",
    "\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f0cd37",
   "metadata": {},
   "source": [
    "### Event parsing\n",
    "- For each event (registration, arrivel, triage, start treatment, depart):\n",
    "  - If both date and time are missing, omit that event for that case (do not add an event row).\n",
    "  - If exactly one of date/time is missing, drop the entire case (partial timestamp).\n",
    "  - If both are present but the combined timestamp is invalid, drop the entire case.\n",
    "- After enforcing the rules above, construct one row per valid event with a parsed timestamp and attach case attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ade6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse events, drop cases on partial/invalid, and build event frames (robust date parsing + diagnostics)\n",
    "\n",
    "# Define event timestamp columns (as provided in the raw data)\n",
    "events = [\n",
    "    {\"name\": \"registration\", \"date\": \"RegitrationDate\", \"time\": \"RegitrationTime\"},\n",
    "    {\"name\": \"arrivel\",       \"date\": \"ArrivelDate\",       \"time\": \"ArrivelTime\"},\n",
    "    {\"name\": \"triage\",        \"date\": \"TriageDate\",        \"time\": \"TriageTime\"},\n",
    "    {\"name\": \"start treatment\",\"date\": \"Start Treatment Date\",\"time\": \"Start TreatmentTime\"},\n",
    "    {\"name\": \"depart\",        \"date\": \"DepartDate\",        \"time\": \"DepartTime\"},\n",
    "]\n",
    "\n",
    "# Columns used only to build event timestamps (will be dropped later)\n",
    "raw_event_cols = [\n",
    "    \"RegitrationDate\",\"RegitrationTime\",\n",
    "    \"ArrivelDate\",\"ArrivelTime\",\n",
    "    \"TriageDate\",\"TriageTime\",\n",
    "    \"Start Treatment Date\",\"Start TreatmentTime\",\n",
    "    \"DepartDate\",\"DepartTime\",\n",
    "]\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def _is_empty(series: pd.Series) -> pd.Series:\n",
    "    return series.astype(str).str.strip().str.lower().isin([\"\", \"nan\", \"nat\", \"null\", \"none\"])\n",
    "\n",
    "\n",
    "def _normalize_date_to_ymd(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Parse any parseable date-like string (even if it contains time) and return YYYY-MM-DD strings.\n",
    "    Unparseable entries become NA (to be handled by caller).\"\"\"\n",
    "    dt = pd.to_datetime(series, errors=\"coerce\", yearfirst=True)\n",
    "    return dt.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def _normalize_time(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Lightly normalize time strings: replace '.' with ':', convert HHMM -> HH:MM; keep others as-is.\"\"\"\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.str.replace(\".\", \":\", regex=False)\n",
    "    # If exactly 4 digits (HHMM), convert to HH:MM\n",
    "    s = s.str.replace(r\"^(\\d{2})(\\d{2})$\", r\"\\1:\\2\", regex=True)\n",
    "    return s\n",
    "\n",
    "# 1) Enforce drop rules: partial (one missing) or invalid parse -> drop entire case.\n",
    "#    If both missing, we simply omit that event later (do not drop case).\n",
    "\n",
    "cases_to_drop = set()\n",
    "for spec in events:\n",
    "    dcol, tcol = spec.get(\"date\"), spec.get(\"time\")\n",
    "\n",
    "    # Only enforce row-level rules when both columns exist\n",
    "    if dcol in df.columns and tcol in df.columns:\n",
    "        date_s = df[dcol]\n",
    "        time_s = df[tcol]\n",
    "        date_empty = _is_empty(date_s)\n",
    "        time_empty = _is_empty(time_s)\n",
    "        both_empty = date_empty & time_empty\n",
    "        partial_mask = date_empty ^ time_empty\n",
    "\n",
    "        # Diagnostics: empties and partials\n",
    "        n_date_empty = int(date_empty.sum())\n",
    "        n_time_empty = int(time_empty.sum())\n",
    "        n_both_empty = int(both_empty.sum())\n",
    "        n_partial = int(partial_mask.sum())\n",
    "\n",
    "        # Partial -> drop cases\n",
    "        if n_partial > 0:\n",
    "            dropped_cases = df.loc[partial_mask, case_id_col].dropna().unique().tolist()\n",
    "            cases_to_drop.update(dropped_cases)\n",
    "\n",
    "        # Both present -> parse normalized date + time; invalid -> drop cases\n",
    "        both_present = (~date_empty) & (~time_empty)\n",
    "        if both_present.any():\n",
    "            date_norm = _normalize_date_to_ymd(date_s)\n",
    "            time_norm = _normalize_time(time_s)\n",
    "            invalid_date = date_norm.isna()\n",
    "            dt_str = (date_norm.fillna(\"\") + \" \" + time_norm.fillna(\"\")).str.strip()\n",
    "            ts = pd.to_datetime(dt_str.replace({\"\": pd.NA}), errors=\"coerce\", yearfirst=True)\n",
    "            invalid_combined = ts.isna()\n",
    "            invalid_mask = both_present & (invalid_date | invalid_combined)\n",
    "\n",
    "            # Diagnostics: invalids when both present\n",
    "            n_both_present = int(both_present.sum())\n",
    "            n_invalid_rows = int(invalid_mask.sum())\n",
    "            cases_invalid = int(df.loc[invalid_mask, case_id_col].dropna().nunique())\n",
    "            print(\n",
    "                f\"[{spec['name']}] both_present rows={n_both_present}, invalid_combined rows={n_invalid_rows} (cases={cases_invalid})\"\n",
    "            )\n",
    "\n",
    "            if n_invalid_rows > 0:\n",
    "                invalid_cases = df.loc[invalid_mask, case_id_col].dropna().unique().tolist()\n",
    "                cases_to_drop.update(invalid_cases)\n",
    "    else:\n",
    "        # Columns not both present: treat as if event is globally missing -> do nothing, will be omitted\n",
    "        missing = [c for c in [dcol, tcol] if c not in df.columns]\n",
    "        print(f\"[{spec['name']}] skipping drop checks (missing column(s): {missing}))\")\n",
    "\n",
    "n_cases_before = df[case_id_col].nunique()\n",
    "if cases_to_drop:\n",
    "    df = df[~df[case_id_col].isin(cases_to_drop)].copy()\n",
    "    print(\n",
    "        f\"Dropped {len(cases_to_drop)} case(s) for partial/invalid timestamps. Remaining cases: {df[case_id_col].nunique()} (from {n_cases_before}).\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No cases dropped for partial/invalid timestamps across events.\")\n",
    "\n",
    "# 2) Build per-activity event frames from the filtered df — keep only valid timestamps\n",
    "\n",
    "log_frames = []\n",
    "trace_attr_cols = [c for c in df.columns if c != case_id_col and c not in raw_event_cols]\n",
    "\n",
    "for spec in events:\n",
    "    dcol, tcol = spec.get(\"date\"), spec.get(\"time\")\n",
    "\n",
    "    # Must have both columns to construct an event\n",
    "    if not (dcol in df.columns and tcol in df.columns):\n",
    "        print(f\"[{spec['name']}] no event built (missing date/time column)\")\n",
    "        continue\n",
    "\n",
    "    date_s = df[dcol]\n",
    "    time_s = df[tcol]\n",
    "    date_empty = _is_empty(date_s)\n",
    "    time_empty = _is_empty(time_s)\n",
    "\n",
    "    # Only rows where both present\n",
    "    both_present = (~date_empty) & (~time_empty)\n",
    "    if not both_present.any():\n",
    "        print(f\"[{spec['name']}] no event rows (no rows with both date and time present)\")\n",
    "        continue\n",
    "\n",
    "    date_norm = _normalize_date_to_ymd(date_s)\n",
    "    time_norm = _normalize_time(time_s)\n",
    "\n",
    "    # Build combined string and parse\n",
    "    dt_str = (date_norm.fillna(\"\") + \" \" + time_norm.fillna(\"\")).str.strip()\n",
    "    ts = pd.to_datetime(dt_str.replace({\"\": pd.NA}), errors=\"coerce\", yearfirst=True)\n",
    "\n",
    "    valid_mask = both_present & date_norm.notna() & ts.notna()\n",
    "    n_kept = int(valid_mask.sum())\n",
    "    if n_kept == 0:\n",
    "        print(f\"[{spec['name']}] no valid timestamps after normalization/parsing\")\n",
    "        continue\n",
    "\n",
    "    ev_df = pd.DataFrame({\n",
    "        case_id_col: df.loc[valid_mask, case_id_col],\n",
    "        \"activity\": spec[\"name\"],\n",
    "        \"timestamp\": ts[valid_mask],\n",
    "    })\n",
    "\n",
    "    # Attach trace attributes (repeat across events), aligning with valid rows\n",
    "    ev_df = pd.concat([ev_df, df.loc[valid_mask, trace_attr_cols]], axis=1)\n",
    "\n",
    "    print(f\"[{spec['name']}] kept {n_kept} event row(s)\")\n",
    "    log_frames.append(ev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03464e2c",
   "metadata": {},
   "source": [
    "### Sorting and cleanup\n",
    "- Sort events within each case by timestamp (ascending).\n",
    "- If timestamps are equal, break ties using the default activity order:\n",
    "  registration < arrivel < triage < start treatment < depart.\n",
    "- Drop any index-like columns (e.g., Unnamed, index, idx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate frames and sort with explicit precedence for ties\n",
    "if not log_frames:\n",
    "    raise RuntimeError(\"No events could be constructed; check exact column names for date/time fields.\")\n",
    "\n",
    "log_df = pd.concat(log_frames, axis=0, ignore_index=True)\n",
    "\n",
    "activity_order = {\n",
    "    \"registration\": 0,\n",
    "    \"arrivel\": 1,\n",
    "    \"triage\": 2,\n",
    "    \"start treatment\": 3,\n",
    "    \"depart\": 4,\n",
    "}\n",
    "log_df[\"_act_order\"] = log_df[\"activity\"].map(activity_order).fillna(99)\n",
    "log_df.sort_values(by=[case_id_col, \"timestamp\", \"_act_order\", \"activity\"], inplace=True)\n",
    "log_df.drop(columns=[\"_act_order\"], inplace=True)\n",
    "\n",
    "# Cleanup: drop index-like columns if any propagated\n",
    "idx_like_cols = [c for c in log_df.columns if str(c).lower().startswith(\"unnamed\") or str(c).lower() in (\"index\", \"idx\")]\n",
    "if idx_like_cols:\n",
    "    print(f\"Dropping index-like columns from event log: {idx_like_cols}\")\n",
    "    log_df.drop(columns=idx_like_cols, inplace=True)\n",
    "    \n",
    "log_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Preview\n",
    "display(log_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c52a9",
   "metadata": {},
   "source": [
    "### Variants\n",
    "- Compute per-case activity sequences from the current event log.\n",
    "- Count variant frequencies and show the top ones with percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for variant analysis (reused)\n",
    "\n",
    "def _compress_consecutive(seq):\n",
    "    out = []\n",
    "    last = object()\n",
    "    for x in seq:\n",
    "        if x != last:\n",
    "            out.append(x)\n",
    "            last = x\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_variant_counts(log_df, case_id_col, activity_col='activity', activity_tie_order=None, compress=True):\n",
    "    \"\"\"\n",
    "    Returns variant_counts DataFrame with columns: ['variant', 'cases'] and a Series seq_per_case.\n",
    "    - Sorts by (case, timestamp, tie-order, activity) if tie-order provided.\n",
    "    - Optionally compresses consecutive duplicate activities within a case before building sequences.\n",
    "    \"\"\"\n",
    "    df = log_df\n",
    "    if activity_tie_order is not None:\n",
    "        df = (df.assign(_act_order=df[activity_col].str.lower().map(activity_tie_order).fillna(99))\n",
    "                .sort_values([case_id_col, 'timestamp', '_act_order', activity_col])\n",
    "                .drop(columns=['_act_order']))\n",
    "    else:\n",
    "        df = df.sort_values([case_id_col, 'timestamp', activity_col])\n",
    "\n",
    "    if compress:\n",
    "        seq_per_case = df.groupby(case_id_col, sort=False)[activity_col] \\\n",
    "            .apply(lambda s: ' > '.join(_compress_consecutive(s.tolist())))\n",
    "    else:\n",
    "        seq_per_case = df.groupby(case_id_col, sort=False)[activity_col] \\\n",
    "            .apply(lambda s: ' > '.join(s.tolist()))\n",
    "\n",
    "    variant_counts = (seq_per_case.value_counts()\n",
    "                                  .rename_axis('variant')\n",
    "                                  .reset_index(name='cases'))\n",
    "    return variant_counts, seq_per_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9674c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variants: compute per-case sequences and frequencies using shared helper\n",
    "\n",
    "variant_counts, seq_per_case = compute_variant_counts(\n",
    "    log_df,\n",
    "    case_id_col=case_id_col,\n",
    "    activity_col='activity',\n",
    "    activity_tie_order={\n",
    "        'registration': 0,\n",
    "        'arrivel': 1,\n",
    "        'triage': 2,\n",
    "        'start treatment': 3,\n",
    "        'depart': 4,\n",
    "    },\n",
    "    compress=True,\n",
    ")\n",
    "\n",
    "total_cases = int(seq_per_case.shape[0])\n",
    "variant_counts['percent_of_cases'] = (variant_counts['cases'] / max(1, total_cases) * 100).round(2)\n",
    "\n",
    "print(f\"Total cases: {total_cases}\")\n",
    "print(f\"Total unique variants: {len(variant_counts)}\")\n",
    "TOP_N = 20\n",
    "print(f\"Top {min(TOP_N, len(variant_counts))} variants by cases (with coverage totals):\")\n",
    "\n",
    "# Prepare top-N with a totals row\n",
    "_top_n = min(TOP_N, len(variant_counts))\n",
    "top_df = variant_counts.head(_top_n).copy()\n",
    "coverage_cases = int(top_df['cases'].sum())\n",
    "coverage_pct = round(coverage_cases / max(1, total_cases) * 100, 2)\n",
    "totals_row = pd.DataFrame([\n",
    "    {\n",
    "        'variant': f'TOTAL (top {_top_n})',\n",
    "        'cases': coverage_cases,\n",
    "        'percent_of_cases': coverage_pct,\n",
    "    }\n",
    "])\n",
    "\n",
    "display(pd.concat([top_df, totals_row], ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcaaa04",
   "metadata": {},
   "source": [
    "# Adding orders log\n",
    "- Use the Orders data to construct additional event rows per case.\n",
    "- Normalize order date/time like the base log: date → YYYY-MM-DD; time: replace '.' with ':' and convert HHMM → HH:MM.\n",
    "- Omit rows where both date and time are missing; drop rows with partial or invalid timestamps.\n",
    "- First, display a frequency table of order activities and optionally filter by minimum occurrences (parameters in the next code cell).\n",
    "- Then, merge the prefiltered order events into the current event log and re-sort with tie-breakers (orders between triage and start treatment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e07572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rare variants before integrating orders\n",
    "# Keep only cases whose pre-orders variant appears in at least this many cases\n",
    "MIN_VARIANT_CASES = 4\n",
    "\n",
    "# Build per-case sequences using current row order (pre-orders log_df)\n",
    "seq_per_case = log_df.groupby(case_id_col, sort=False)['activity'].apply(lambda s: ' > '.join(s.tolist()))\n",
    "variant_counts = (seq_per_case.value_counts()\n",
    "                               .rename_axis('variant')\n",
    "                               .reset_index(name='cases'))\n",
    "\n",
    "# Determine which variants to drop\n",
    "variants_to_drop = set(variant_counts.loc[variant_counts['cases'] < int(MIN_VARIANT_CASES), 'variant'].tolist())\n",
    "\n",
    "print(f\"Pre-orders variants total: {len(variant_counts)}\")\n",
    "print(f\"Variants to drop (< {MIN_VARIANT_CASES} cases): {len(variants_to_drop)}\")\n",
    "\n",
    "# Map case -> variant and filter cases\n",
    "case_to_variant = seq_per_case\n",
    "allowed_cases = set(case_to_variant[~case_to_variant.isin(variants_to_drop)].index)\n",
    "\n",
    "before_cases = int(log_df[case_id_col].nunique())\n",
    "before_events = int(len(log_df))\n",
    "\n",
    "filtered_df = log_df[log_df[case_id_col].isin(allowed_cases)].copy()\n",
    "kept_cases = int(filtered_df[case_id_col].nunique())\n",
    "kept_events = int(len(filtered_df))\n",
    "\n",
    "print(\n",
    "    f\"Kept {kept_cases}/{before_cases} cases \"\n",
    "    f\"({(kept_cases/before_cases*100 if before_cases else 0):.2f}%), \"\n",
    "    f\"{kept_events}/{before_events} events \"\n",
    "    f\"({(kept_events/before_events*100 if before_events else 0):.2f}%).\"\n",
    ")\n",
    "\n",
    "# Update global log\n",
    "log_df = filtered_df\n",
    "\n",
    "display(variant_counts.tail(5))  # show the bottom variants for reference\n",
    "print(\"log_df has been filtered by pre-orders variant frequency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882cd528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orders activity frequency (before filtering and integration)\n",
    "# Build a valid orders subset consistent with base parsing and show activity counts.\n",
    "# Keep only the TOP-K most frequent order activities for integration.\n",
    "\n",
    "# Parameter: number of top activities to keep (by frequency)\n",
    "ORDERS_TOP_K_ACTIVITIES = 30  # change as needed\n",
    "\n",
    "# Column names in Orders\n",
    "orders_case_col = 'SEH_KoppelID'\n",
    "orders_date_col = 'Order_Startdate'\n",
    "orders_time_col = 'order_starttime'\n",
    "orders_name_col = 'SEH_orders_clean' if 'SEH_orders_clean' in orders_df.columns else ('SEH_orders' if 'SEH_orders' in orders_df.columns else None)\n",
    "if orders_name_col is None:\n",
    "    orders_df = orders_df.copy()\n",
    "    orders_name_col = '_order_name_'\n",
    "    orders_df[orders_name_col] = 'ORDER'\n",
    "\n",
    "# Build masks using the same helpers as the base parsing\n",
    "_date_s = orders_df[orders_date_col]\n",
    "_time_s = orders_df[orders_time_col]\n",
    "_date_empty = _is_empty(_date_s)\n",
    "_time_empty = _is_empty(_time_s)\n",
    "\n",
    "_both_present = (~_date_empty) & (~_time_empty)\n",
    "_date_norm = _normalize_date_to_ymd(_date_s)\n",
    "_time_norm = _normalize_time(_time_s)\n",
    "_dt_str = (_date_norm.fillna('') + ' ' + _time_norm.fillna('')).str.strip()\n",
    "_ts = pd.to_datetime(_dt_str.replace({'': pd.NA}), errors='coerce', yearfirst=True)\n",
    "_invalid = _both_present & (_date_norm.isna() | _ts.isna())\n",
    "\n",
    "# Keep only valid orders with case ids present in the base log\n",
    "_valid = _both_present & (~_invalid) & orders_df[orders_case_col].isin(set(log_df[case_id_col].dropna().unique()))\n",
    "kept = int(_valid.sum())\n",
    "print(f\"[orders] candidate valid rows for frequency table: {kept}\")\n",
    "\n",
    "if kept == 0:\n",
    "    print(\"No valid orders available for frequency table.\")\n",
    "    # Provide an empty placeholder for downstream merge to consume safely\n",
    "    orders_events_filtered = pd.DataFrame(columns=[case_id_col, 'activity', 'timestamp'])\n",
    "else:\n",
    "    orders_events = pd.DataFrame({\n",
    "        case_id_col: orders_df.loc[_valid, orders_case_col],\n",
    "        'activity': orders_df.loc[_valid, orders_name_col].astype(str).str.strip(),\n",
    "        'timestamp': _ts[_valid],\n",
    "    })\n",
    "    if 'Specialismecode' in orders_df.columns and 'Specialismecode' not in orders_events.columns:\n",
    "        orders_events['Specialismecode'] = orders_df.loc[_valid, 'Specialismecode']\n",
    "\n",
    "    # Frequency BEFORE filtering\n",
    "    orders_counts_pre = (\n",
    "        orders_events['activity']\n",
    "            .value_counts()\n",
    "            .rename_axis('activity')\n",
    "            .reset_index(name='events')\n",
    "    )\n",
    "    print(\"\\nOrders activity frequency (before TOP-K filtering):\")\n",
    "    print(f\"- Unique order activities: {orders_counts_pre.shape[0]}\")\n",
    "    display(orders_counts_pre)\n",
    "\n",
    "    # Keep only the top-K activities\n",
    "    k = int(max(0, ORDERS_TOP_K_ACTIVITIES))\n",
    "    if k == 0:\n",
    "        print(\"\\nORDERS_TOP_K_ACTIVITIES is 0 → keeping no order activities.\")\n",
    "        orders_events_filtered = orders_events.iloc[0:0].copy()\n",
    "    else:\n",
    "        top_activities = orders_counts_pre['activity'].head(k).tolist()\n",
    "        before_events = int(len(orders_events))\n",
    "        before_acts = int(orders_counts_pre.shape[0])\n",
    "\n",
    "        orders_events_filtered = orders_events[orders_events['activity'].isin(top_activities)].copy()\n",
    "        after_events = int(len(orders_events_filtered))\n",
    "        after_acts = int(orders_events_filtered['activity'].nunique())\n",
    "\n",
    "        print(f\"\\nApplied TOP-K filter: keeping top {k} activities by frequency.\")\n",
    "        print(f\"- Activities: {after_acts}/{before_acts} retained; Events: {after_events}/{before_events} retained\")\n",
    "\n",
    "        if after_events > 0:\n",
    "            orders_counts_post = (\n",
    "                orders_events_filtered['activity']\n",
    "                    .value_counts()\n",
    "                    .rename_axis('activity')\n",
    "                    .reset_index(name='events')\n",
    "            )\n",
    "            print(\"\\nOrders activity frequency (after TOP-K filtering):\")\n",
    "            display(orders_counts_post)\n",
    "        else:\n",
    "            print(\"No order events remain after TOP-K filtering.\")\n",
    "\n",
    "    print(f\"\\nPrepared 'orders_events_filtered' with {len(orders_events_filtered)} row(s) for integration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Orders timestamps like the base log, then combine into log_df\n",
    "\n",
    "# This cell merges prefiltered orders (from the previous cell) into the event log.\n",
    "# If the previous cell wasn't run, we'll fall back to computing valid orders without filtering.\n",
    "\n",
    "# Column names in Orders\n",
    "orders_case_col = 'SEH_KoppelID'\n",
    "orders_date_col = 'Order_Startdate'\n",
    "orders_time_col = 'order_starttime'\n",
    "orders_name_col = 'SEH_orders_clean' if 'SEH_orders_clean' in orders_df.columns else ('SEH_orders' if 'SEH_orders' in orders_df.columns else None)\n",
    "if orders_name_col is None:\n",
    "    orders_df = orders_df.copy()\n",
    "    orders_name_col = '_order_name_'\n",
    "    orders_df[orders_name_col] = 'ORDER'\n",
    "\n",
    "# Use prefiltered orders if available, otherwise compute valid orders quickly\n",
    "if 'orders_events_filtered' in globals():\n",
    "    orders_events = orders_events_filtered.copy()\n",
    "    print(f\"[orders] using prefiltered order events: {len(orders_events)} row(s)\")\n",
    "else:\n",
    "    # Build masks using the same helpers as the base parsing\n",
    "    _date_s = orders_df[orders_date_col]\n",
    "    _time_s = orders_df[orders_time_col]\n",
    "    _date_empty = _is_empty(_date_s)\n",
    "    _time_empty = _is_empty(_time_s)\n",
    "    _both_present = (~_date_empty) & (~_time_empty)\n",
    "    _date_norm = _normalize_date_to_ymd(_date_s)\n",
    "    _time_norm = _normalize_time(_time_s)\n",
    "    _dt_str = (_date_norm.fillna('') + ' ' + _time_norm.fillna('')).str.strip()\n",
    "    _ts = pd.to_datetime(_dt_str.replace({'': pd.NA}), errors='coerce', yearfirst=True)\n",
    "    _invalid = _both_present & (_date_norm.isna() | _ts.isna())\n",
    "\n",
    "    _valid = _both_present & (~_invalid) & orders_df[orders_case_col].isin(set(log_df[case_id_col].dropna().unique()))\n",
    "    kept = int(_valid.sum())\n",
    "    print(f\"[orders] kept {kept} valid order event row(s) (no prefiltered set found)\")\n",
    "\n",
    "    orders_events = pd.DataFrame({\n",
    "        case_id_col: orders_df.loc[_valid, orders_case_col],\n",
    "        'activity': orders_df.loc[_valid, orders_name_col].astype(str).str.strip(),\n",
    "        'timestamp': _ts[_valid],\n",
    "    })\n",
    "    if 'Specialismecode' in orders_df.columns and 'Specialismecode' not in orders_events.columns:\n",
    "        orders_events['Specialismecode'] = orders_df.loc[_valid, 'Specialismecode']\n",
    "\n",
    "# If nothing to merge, show the base log and exit\n",
    "if orders_events.empty:\n",
    "    print(\"No orders to integrate; keeping base event log unchanged.\")\n",
    "    display(log_df.head(10))\n",
    "else:\n",
    "    # Combine and sort with precedence: orders between triage and start treatment\n",
    "    combined = pd.concat([log_df.assign(_is_order=False), orders_events.assign(_is_order=True)], ignore_index=True, sort=False)\n",
    "    _precedence = {'registration': 0, 'arrivel': 1, 'triage': 2, 'start treatment': 4, 'depart': 5}\n",
    "    combined['_order_rank'] = combined.apply(lambda r: 3 if bool(r.get('_is_order', False)) else _precedence.get(str(r.get('activity')).lower(), 99), axis=1)\n",
    "    combined.sort_values(by=[case_id_col, 'timestamp', '_order_rank', 'activity'], inplace=True)\n",
    "    combined.drop(columns=['_order_rank', '_is_order'], inplace=True, errors='ignore')\n",
    "    combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Deduplicate exact duplicate events (same case, activity, timestamp)\n",
    "    _dup_mask = combined.duplicated(subset=[case_id_col, 'activity', 'timestamp'], keep='first')\n",
    "    _num_dups = int(_dup_mask.sum())\n",
    "    if _num_dups > 0:\n",
    "        print(f\"Removed {_num_dups} exact duplicate event row(s) (same case, activity, timestamp).\")\n",
    "        combined = combined[~_dup_mask].copy()\n",
    "\n",
    "    # Fill NaNs in attributes from the registration event of the same case id\n",
    "    reg_rows = combined[combined['activity'] == 'registration']\n",
    "    if not reg_rows.empty:\n",
    "        reg_by_case = reg_rows.set_index(case_id_col)\n",
    "        cols_to_fill = [c for c in combined.columns if c not in [case_id_col, 'activity', 'timestamp']]\n",
    "        _filled_total = 0\n",
    "        for col in cols_to_fill:\n",
    "            if col in reg_by_case.columns:\n",
    "                _before_missing = int(combined[col].isna().sum())\n",
    "                combined[col] = combined[col].fillna(combined[case_id_col].map(reg_by_case[col]))\n",
    "                _after_missing = int(combined[col].isna().sum())\n",
    "                _filled_total += (_before_missing - _after_missing)\n",
    "        if _filled_total > 0:\n",
    "            print(f\"Filled {_filled_total} missing attribute value(s) from registration event attributes.\")\n",
    "\n",
    "    # Update log_df\n",
    "    log_df = combined\n",
    "    print(f\"Orders integrated. New event log size: {len(log_df)} events for {log_df[case_id_col].nunique()} cases.\")\n",
    "    display(log_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af7d08e",
   "metadata": {},
   "source": [
    "### Variants (after orders integration)\n",
    "- The event log has changed after integrating Orders.\n",
    "- Below we compute per-case activity sequences from the current, combined `log_df` and show the top variants with percentages.\n",
    "- This snapshot is taken right after Orders are merged, and before assigning resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf93c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variants after orders integration: compute per-case sequences and frequencies on the updated log\n",
    "print(\"Variants (after orders integration)\")\n",
    "\n",
    "variant_counts, seq_per_case = compute_variant_counts(\n",
    "    log_df,\n",
    "    case_id_col=case_id_col,\n",
    "    activity_col='activity',\n",
    "    activity_tie_order={\n",
    "        'registration': 0,\n",
    "        'arrivel': 1,\n",
    "        'triage': 2,\n",
    "        # Orders effectively rank 3 via integration sort; tie-break handled earlier\n",
    "        'start treatment': 4,\n",
    "        'depart': 5,\n",
    "    },\n",
    "    compress=True,\n",
    ")\n",
    "\n",
    "total_cases = int(seq_per_case.shape[0])\n",
    "variant_counts['percent_of_cases'] = (variant_counts['cases'] / max(1, total_cases) * 100).round(2)\n",
    "\n",
    "print(f\"Total cases: {total_cases}\")\n",
    "print(f\"Total unique variants (after orders): {len(variant_counts)}\")\n",
    "TOP_SHOW = 20\n",
    "print(f\"Top {min(TOP_SHOW, len(variant_counts))} variants by cases (with coverage totals):\")\n",
    "\n",
    "# Prepare top-N with a totals row\n",
    "_top_n = min(TOP_SHOW, len(variant_counts))\n",
    "top_df = variant_counts.head(_top_n).copy()\n",
    "coverage_cases = int(top_df['cases'].sum())\n",
    "coverage_pct = round(coverage_cases / max(1, total_cases) * 100, 2)\n",
    "totals_row = pd.DataFrame([\n",
    "    {\n",
    "        'variant': f'TOTAL (top {_top_n})',\n",
    "        'cases': coverage_cases,\n",
    "        'percent_of_cases': coverage_pct,\n",
    "    }\n",
    "])\n",
    "\n",
    "display(pd.concat([top_df, totals_row], ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6273014",
   "metadata": {},
   "source": [
    "# Assign resources\n",
    "Assign deterministic nurse/doctor resources per event using shift and weekend patterns; keep assignments stable via a seeded RNG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a4c1ed",
   "metadata": {},
   "source": [
    "### Define activity roles\n",
    "- Below we list all activities present in the current `log_df` and assign a role to each: `nurse` or `doctor`.\n",
    "- A default heuristic is used (e.g., \"start treatment\" → doctor; everything else → nurse). You can edit the mapping.\n",
    "- The resulting `activity_role` dict will be used by the resource assignment step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49208348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List activities and assign a role per activity (doctor or nurse)\n",
    "\n",
    "# Collect unique activities as they appear in the log\n",
    "activities = (pd.Series(log_df['activity'])\n",
    "                .dropna()\n",
    "                .astype(str)\n",
    "                .str.strip()\n",
    "                .unique()\n",
    "                .tolist())\n",
    "activities = sorted(activities)\n",
    "\n",
    "print(f\"Found {len(activities)} activities:\")\n",
    "for a in activities:\n",
    "    print(f\" - {a}\")\n",
    "\n",
    "# Helper to normalize text (remove accents, lowercase, strip)\n",
    "\n",
    "def _clean(text: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKD\", str(text)).encode(\"ascii\", \"ignore\").decode(\"ascii\").lower().strip()\n",
    "\n",
    "# Default heuristic: mark doctor-driven activities by keywords (EN/NL medical terms)\n",
    "# - You can still override specific activities below with activity_role.update({...})\n",
    "\n",
    "def _default_role_for(activity: str) -> str:\n",
    "    s = _clean(activity)\n",
    "\n",
    "    # Exact doctor-only names\n",
    "    doctor_exact = {\n",
    "        'start treatment',\n",
    "    }\n",
    "\n",
    "    # Substring keywords that strongly indicate a doctor action\n",
    "    doctor_keywords = [\n",
    "        # Clinical decision/assessment/consultations\n",
    "        'consult', 'assessment', 'arts', 'dokter', 'physician', 'doctor',\n",
    "        'beoordel',  # beoordeling (assessment)\n",
    "        'diagnos',    # diagnose/diagnostic\n",
    "        'anamnese',   # anamnesis\n",
    "        'onderzoek',  # examination\n",
    "\n",
    "        # Procedures/treatments/interventions\n",
    "        'procedure', 'operat', 'ingreep', 'hecht', 'sutur', 'reduct',\n",
    "        'intubat', 'reanim', 'resusc', 'immobil', 'gips', 'cast', 'splint',\n",
    "\n",
    "        # Prescriptions/referrals/orders (typically initiated by doctor)\n",
    "        'verwij', 'referr', 'prescrib', 'recept', 'medicat', 'order',\n",
    "\n",
    "        # Imaging and diagnostics\n",
    "        'x-ray', 'xray', 'rontg', 'radiolog', 'ct', 'mri', 'echo', 'ultra', 'ecg', 'ekg',\n",
    "\n",
    "        # Laboratory\n",
    "        'lab', 'bloed', 'blood', 'urine', 'kweek', 'culture', 'crp', 'troponin',\n",
    "    ]\n",
    "\n",
    "    if s in doctor_exact or any(kw in s for kw in doctor_keywords):\n",
    "        return 'doctor'\n",
    "    return 'nurse'\n",
    "\n",
    "# Build mapping for all activities\n",
    "activity_role = {a: _default_role_for(a) for a in activities}\n",
    "\n",
    "# Example overrides (uncomment and adapt):\n",
    "# activity_role.update({\n",
    "#     'specific activity name': 'doctor',\n",
    "#     'another activity': 'nurse',\n",
    "# })\n",
    "\n",
    "print(\"\\nAssigned roles:\")\n",
    "for a in activities:\n",
    "    print(f\" - {a}: {activity_role[a]}\")\n",
    "\n",
    "print(\"\\nSummary by role:\")\n",
    "print(dict(Counter(activity_role.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384dd07a",
   "metadata": {},
   "source": [
    "### Activity counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733832b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event counts by activity and by role\n",
    "import pandas as pd\n",
    "\n",
    "# Preconditions\n",
    "if 'log_df' not in globals():\n",
    "    raise RuntimeError(\"Event log 'log_df' not found. Please run the previous steps first.\")\n",
    "if 'activity_role' not in globals():\n",
    "    raise RuntimeError(\"'activity_role' not defined. Please run the 'Define activity roles' cell before this one.\")\n",
    "\n",
    "# Total events\n",
    "total_events = int(len(log_df))\n",
    "print(f\"Total events: {total_events}\")\n",
    "\n",
    "# Counts by activity\n",
    "events_by_activity = (log_df['activity']\n",
    "                      .value_counts()\n",
    "                      .rename_axis('activity')\n",
    "                      .reset_index(name='events'))\n",
    "if total_events:\n",
    "    events_by_activity['percent_of_events'] = (events_by_activity['events'] / total_events * 100).round(2)\n",
    "\n",
    "# Role series from mapping\n",
    "role_s = log_df['activity'].map(lambda a: activity_role.get(a, 'nurse')).rename('role')\n",
    "\n",
    "# Counts by role\n",
    "events_by_role = (role_s.value_counts()\n",
    "                  .rename_axis('role')\n",
    "                  .reset_index(name='events'))\n",
    "if total_events:\n",
    "    events_by_role['percent_of_events'] = (events_by_role['events'] / total_events * 100).round(2)\n",
    "\n",
    "print(\"\\nEvents by activity (top 25 shown):\")\n",
    "display(events_by_activity.head(25))\n",
    "\n",
    "print(\"\\nEvents by role:\")\n",
    "display(events_by_role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d1848",
   "metadata": {},
   "source": [
    "# Staffing rationale and seasonal patterns\n",
    "\n",
    "We size the number of nurse and doctor resources from observed demand and simple, transparent assumptions:\n",
    "\n",
    "- Compute the time coverage of the log (from min to max timestamp) and average events/day by role.\n",
    "- Set target throughput per staff per shift:\n",
    "  - Nurses: 15 events/shift (triage, registration, orders support, etc.).\n",
    "  - Doctors: 8 events/shift (assessments, procedures, orders initiation).\n",
    "- Estimate baseline staff-per-shift = ceil(events_per_day / target_per_shift).\n",
    "- Account for seasonal peaks: winter +20% demand, summer −15%, spring/autumn baseline.\n",
    "- Recommend roster size as the peak staff-per-shift × number of shifts (3), with a small buffer.\n",
    "\n",
    "Assignment patterns in the next cell reflect seasonality and shifts:\n",
    "- Three shifts: day [07–15), evening [15–23), night [23–07).\n",
    "- Weekends bias toward higher doctor coverage at night/evening.\n",
    "- Seasonal active-roster fractions (e.g., a larger fraction of the roster active in winter), rotating weekly so names vary naturally.\n",
    "\n",
    "You can override the recommended counts by setting `n_nurses` / `n_doctors` before the generation cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute recommended staffing from demand and set defaults\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# Preconditions\n",
    "if 'log_df' not in globals():\n",
    "    raise RuntimeError(\"Event log 'log_df' not found. Please run previous steps.\")\n",
    "if 'activity_role' not in globals():\n",
    "    raise RuntimeError(\"'activity_role' not defined. Please run the 'Define activity roles' cell.\")\n",
    "\n",
    "# Role series and counts\n",
    "role_s = pd.Series(log_df['activity']).map(lambda a: activity_role.get(a, 'nurse')).rename('role')\n",
    "role_counts = role_s.value_counts()\n",
    "nurse_events = int(role_counts.get('nurse', 0))\n",
    "doctor_events = int(role_counts.get('doctor', 0))\n",
    "\n",
    "# Coverage window\n",
    "_ts = pd.to_datetime(log_df['timestamp'], errors='coerce')\n",
    "min_ts, max_ts = _ts.min(), _ts.max()\n",
    "if pd.isna(min_ts) or pd.isna(max_ts):\n",
    "    raise RuntimeError(\"Timestamps missing; cannot compute coverage window.\")\n",
    "days = max(1, int((max_ts.normalize() - min_ts.normalize()).days) + 1)\n",
    "\n",
    "# Events/day by role\n",
    "nurse_epd = nurse_events / days\n",
    "doctor_epd = doctor_events / days\n",
    "\n",
    "# Throughput targets (events per staff per shift)\n",
    "NURSE_TARGET_PER_SHIFT = 15\n",
    "DOCTOR_TARGET_PER_SHIFT = 8\n",
    "\n",
    "# Baseline staff-per-shift\n",
    "nurse_staff_shift = math.ceil(nurse_epd / max(1, NURSE_TARGET_PER_SHIFT))\n",
    "doctor_staff_shift = math.ceil(doctor_epd / max(1, DOCTOR_TARGET_PER_SHIFT))\n",
    "\n",
    "# Peak multipliers to cover winter; we size to peak\n",
    "NURSE_PEAK_FACTOR = 1.20\n",
    "DOCTOR_PEAK_FACTOR = 1.15\n",
    "\n",
    "nurse_staff_shift_peak = max(1, math.ceil(nurse_staff_shift * NURSE_PEAK_FACTOR))\n",
    "doctor_staff_shift_peak = max(1, math.ceil(doctor_staff_shift * DOCTOR_PEAK_FACTOR))\n",
    "\n",
    "# Recommend roster sizes (3 shifts for nurses; 2 for doctors typical)\n",
    "recommended_n_nurses = max(6, nurse_staff_shift_peak * 3)\n",
    "recommended_n_doctors = max(3, doctor_staff_shift_peak * 2)\n",
    "\n",
    "print(\"Demand-driven staffing suggestion:\")\n",
    "print(f\"- Coverage: {days} day(s) from {min_ts.date()} to {max_ts.date()}\")\n",
    "print(f\"- Nurse events: {nurse_events:,}  (~{nurse_epd:.1f}/day)\")\n",
    "print(f\"- Doctor events: {doctor_events:,} (~{doctor_epd:.1f}/day)\")\n",
    "print(f\"- Baseline staff/shift → nurses={nurse_staff_shift}, doctors={doctor_staff_shift}\")\n",
    "print(f\"- Peak-adjusted staff/shift → nurses={nurse_staff_shift_peak}, doctors={doctor_staff_shift_peak}\")\n",
    "print(f\"- Recommended unique names → nurses={recommended_n_nurses}, doctors={recommended_n_doctors}\")\n",
    "\n",
    "print(\"The generation step will use the recommended values by default; define n_nurses/n_doctors only if you explicitly want to override.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95233bbb",
   "metadata": {},
   "source": [
    "### Generate Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c569f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nurses = int(globals().get('recommended_n_nurses', globals().get('n_nurses', 6)))\n",
    "n_doctors = int(globals().get('recommended_n_doctors', globals().get('n_doctors', 3)))\n",
    "\n",
    "# -----------------------------\n",
    "# Kept knobs (no season needed)\n",
    "# -----------------------------\n",
    "RHO_TARGET = 0.80  # target utilization\n",
    "SERVICE_RATE_PER_ROLE = {  # completions/hour per resource\n",
    "    \"nurse\": 6.0,\n",
    "    \"doctor\": 3.0,\n",
    "}\n",
    "TAU_BY_SHIFT = {\"day\": 1.2, \"evening\": 1.0, \"night\": 0.7, \"unknown\": 1.0}\n",
    "CONTINUITY_GAMMA = 2.0\n",
    "SHIFT_STEP = {\"day\": 3, \"evening\": 5, \"night\": 7, \"unknown\": 3}  # for full-roster rotation\n",
    "\n",
    "# Helper: generate alphabetic suffixes: A..Z, AA..ZZ, AAA..\n",
    "from typing import List\n",
    "def _alpha_suffixes(n: int) -> List[str]:\n",
    "    def _to_letters(idx: int) -> str:\n",
    "        s, x = \"\", idx + 1\n",
    "        while x > 0:\n",
    "            x, rem = divmod(x - 1, 26)\n",
    "            s = chr(ord('A') + rem) + s\n",
    "        return s\n",
    "    return [_to_letters(i) for i in range(max(0, int(n)))]\n",
    "\n",
    "# Names\n",
    "nurse_names = [f\"nurse_{s}\" for s in _alpha_suffixes(n_nurses)]\n",
    "doctor_names = [f\"doctor_{s}\" for s in _alpha_suffixes(n_doctors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f49a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefer recommended counts; only fall back if not available\n",
    "n_nurses = int(globals().get('recommended_n_nurses', globals().get('n_nurses', 6)))\n",
    "n_doctors = int(globals().get('recommended_n_doctors', globals().get('n_doctors', 3)))\n",
    "\n",
    "# -----------------------------\n",
    "# Kept knobs (no season needed)\n",
    "# -----------------------------\n",
    "RHO_TARGET = 0.80  # target utilization\n",
    "SERVICE_RATE_PER_ROLE = {  # completions/hour per resource\n",
    "    \"nurse\": 6.0,\n",
    "    \"doctor\": 3.0,\n",
    "}\n",
    "TAU_BY_SHIFT = {\"day\": 1.2, \"evening\": 1.0, \"night\": 0.7, \"unknown\": 1.0}\n",
    "CONTINUITY_GAMMA = 2.0\n",
    "SHIFT_STEP = {\"day\": 3, \"evening\": 5, \"night\": 7, \"unknown\": 3}  # for full-roster rotation\n",
    "\n",
    "# Helper: generate alphabetic suffixes: A..Z, AA..ZZ, AAA..\n",
    "from typing import List\n",
    "def _alpha_suffixes(n: int) -> List[str]:\n",
    "    def _to_letters(idx: int) -> str:\n",
    "        s, x = \"\", idx + 1\n",
    "        while x > 0:\n",
    "            x, rem = divmod(x - 1, 26)\n",
    "            s = chr(ord('A') + rem) + s\n",
    "        return s\n",
    "    return [_to_letters(i) for i in range(max(0, int(n)))]\n",
    "\n",
    "# Names\n",
    "nurse_names = [f\"nurse_{s}\" for s in _alpha_suffixes(n_nurses)]\n",
    "doctor_names = [f\"doctor_{s}\" for s in _alpha_suffixes(n_doctors)]\n",
    "\n",
    "# Deterministic RNG per event\n",
    "def rng_for_row(row, extra_key: str = \"\"):\n",
    "    date_str = row[\"timestamp\"].strftime(\"%Y-%m-%d\") if pd.notna(row[\"timestamp\"]) else \"NA\"\n",
    "    key = f\"{row[case_id_col]}|{row['activity']}|{date_str}|{extra_key}\"\n",
    "    seed = int(hashlib.sha1(key.encode(\"utf-8\")).hexdigest(), 16) % (2**32)\n",
    "    return np.random.default_rng(seed)\n",
    "\n",
    "# Shift + weekend + week parity (parity kept only for pivot rotation)\n",
    "def shift_and_week(row):\n",
    "    ts = row[\"timestamp\"]\n",
    "    if pd.isna(ts):\n",
    "        return \"unknown\", False, 0\n",
    "    hour = ts.hour\n",
    "    if 7 <= hour < 15:\n",
    "        shift = \"day\"\n",
    "    elif 15 <= hour < 23:\n",
    "        shift = \"evening\"\n",
    "    else:\n",
    "        shift = \"night\"\n",
    "    is_weekend = ts.dayofweek >= 5\n",
    "    week_parity = int(ts.isocalendar().week) % 2\n",
    "    return shift, is_weekend, week_parity\n",
    "\n",
    "# Activity -> role mapping\n",
    "if 'activity_role' not in globals():\n",
    "    activity_role = {\n",
    "        \"registration\": \"nurse\",\n",
    "        \"arrivel\": \"nurse\",\n",
    "        \"triage\": \"nurse\",\n",
    "        \"start treatment\": \"doctor\",\n",
    "        \"depart\": \"nurse\",\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Demand from the actual log\n",
    "# -----------------------------\n",
    "log_df = log_df.sort_values(\"timestamp\")  # IMPORTANT for continuity\n",
    "_tmp = log_df.copy()\n",
    "_tmp[\"shift\"] = _tmp.apply(lambda r: shift_and_week(r)[0], axis=1)\n",
    "_tmp[\"role\"]  = _tmp[\"activity\"].map(lambda a: activity_role.get(a, \"nurse\"))\n",
    "_tmp[\"date\"]  = _tmp[\"timestamp\"].dt.date\n",
    "\n",
    "def _shift_len_hours(shift: str) -> float:\n",
    "    return 8.0  # day/evening/night = 8h\n",
    "\n",
    "demand_counts = (\n",
    "    _tmp.groupby([\"date\", \"shift\", \"role\"])\n",
    "        .size()\n",
    "        .rename(\"tasks\")\n",
    "        .reset_index()\n",
    ")\n",
    "demand_counts[\"lambda_per_hour\"] = demand_counts[\"tasks\"] / demand_counts[\"shift\"].map(_shift_len_hours)\n",
    "DEMAND_LOOKUP = {\n",
    "    (row[\"date\"], row[\"shift\"], row[\"role\"]): float(row[\"lambda_per_hour\"])\n",
    "    for _, row in demand_counts.iterrows()\n",
    "}\n",
    "\n",
    "def demand_lambda(ts: pd.Timestamp, shift: str, role: str) -> float:\n",
    "    \"\"\"Return measured tasks/hour; if missing, assume 0.\"\"\"\n",
    "    if pd.isna(ts):\n",
    "        return 0.0\n",
    "    return DEMAND_LOOKUP.get((ts.date(), shift, role), 0.0)\n",
    "\n",
    "def target_k(n_total: int, role: str, ts: pd.Timestamp, shift: str) -> int:\n",
    "    \"\"\"k = ceil(lambda / (rho*mu)), clipped to [1, n_total].\"\"\"\n",
    "    if n_total <= 0:\n",
    "        return 0\n",
    "    lam = demand_lambda(ts, shift, role)                  # tasks/hour\n",
    "    mu  = SERVICE_RATE_PER_ROLE.get(role, 4.0)            # tasks/hour per resource\n",
    "    k = int(np.ceil(lam / max(1e-9, (RHO_TARGET * mu))))\n",
    "    return int(np.clip(max(1, k), 1, n_total))            # at least 1 on duty\n",
    "\n",
    "# -----------------------------\n",
    "# Full-roster rotation + scatter\n",
    "# -----------------------------\n",
    "def active_indices(n: int, k: int, ts: pd.Timestamp, shift: str) -> list[int]:\n",
    "    if n <= 0 or k <= 0:\n",
    "        return []\n",
    "    if k >= n:\n",
    "        return list(range(n))\n",
    "\n",
    "    week_num = int(ts.isocalendar().week) if pd.notna(ts) else 0\n",
    "    step = SHIFT_STEP.get(shift, 3)\n",
    "    start = (step * week_num) % n\n",
    "    gap = n / k\n",
    "\n",
    "    # Use floor, not round, to stay within [0, n-1]\n",
    "    vals = (start + np.arange(k) * gap) % n\n",
    "    idxs = np.floor(vals).astype(int).tolist()\n",
    "\n",
    "    # Deduplicate (rare when n/k is not integer), then fill forward\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for v in idxs:\n",
    "        if v not in seen:\n",
    "            unique.append(v)\n",
    "            seen.add(v)\n",
    "\n",
    "    cur = start\n",
    "    while len(unique) < k:\n",
    "        cur = (cur + 1) % n\n",
    "        if cur not in seen:\n",
    "            unique.append(int(cur))\n",
    "            seen.add(int(cur))\n",
    "\n",
    "    # Final safety: clamp (should be unnecessary now, but cheap)\n",
    "    return [i % n for i in unique]\n",
    "\n",
    "# -----------------------------\n",
    "# Softmax weights around pivot\n",
    "# -----------------------------\n",
    "def _softmax_weights(m: int, pivot: int, shift: str) -> np.ndarray:\n",
    "    if m <= 0:\n",
    "        return np.array([])\n",
    "    tau = TAU_BY_SHIFT.get(shift, 1.0)\n",
    "    idx = np.arange(m)\n",
    "    d = np.minimum(np.abs(idx - pivot), m - np.abs(idx - pivot))  # circular distance\n",
    "    w = np.exp(-(d ** 2) / (2 * tau * tau))\n",
    "    return w / w.sum()\n",
    "\n",
    "def nurse_weights(n: int, shift: str, is_weekend: bool, week_parity: int) -> np.ndarray:\n",
    "    if n <= 0:\n",
    "        return np.array([])\n",
    "    if shift == \"night\":\n",
    "        pivot = (2 + week_parity) % n if n >= 3 else (week_parity % n)\n",
    "    elif is_weekend:\n",
    "        pivot = (1 + week_parity) % n if n >= 2 else 0\n",
    "    else:\n",
    "        pivot = (0 + week_parity) % n if n >= 2 else 0\n",
    "    return _softmax_weights(n, pivot, shift)\n",
    "\n",
    "def doctor_weights(n: int, shift: str, is_weekend: bool, week_parity: int) -> np.ndarray:\n",
    "    if n <= 0:\n",
    "        return np.array([])\n",
    "    pivot = 0\n",
    "    if n >= 2:\n",
    "        pivot = (1 if (is_weekend or shift == \"night\") else 0)\n",
    "        pivot = (pivot + week_parity) % n\n",
    "    return _softmax_weights(n, pivot, shift)\n",
    "\n",
    "# Weighted choice\n",
    "def pick(pool, weights, rng):\n",
    "    idx = rng.choice(len(pool), p=weights)\n",
    "    return pool[int(idx)]\n",
    "\n",
    "# -----------------------------\n",
    "# Continuity memory\n",
    "# -----------------------------\n",
    "_last_resource_for = {}  # (case_id, role, shift) -> resource_name\n",
    "\n",
    "# Assignment\n",
    "def assign_resource(row):\n",
    "    role = activity_role.get(row[\"activity\"], \"nurse\")\n",
    "    shift, is_weekend, week_parity = shift_and_week(row)\n",
    "    ts = row[\"timestamp\"]\n",
    "\n",
    "    # Determine pool size from measured demand\n",
    "    if role == \"nurse\":\n",
    "        n_total = len(nurse_names)\n",
    "        k = target_k(n_total, role, ts, shift)\n",
    "        idxs = active_indices(n_total, k, ts, shift)\n",
    "        pool = [nurse_names[i] for i in idxs]\n",
    "        w = nurse_weights(len(pool), shift, is_weekend, week_parity)\n",
    "    else:\n",
    "        n_total = len(doctor_names)\n",
    "        k = target_k(n_total, role, ts, shift)\n",
    "        idxs = active_indices(n_total, k, ts, shift)\n",
    "        pool = [doctor_names[i] for i in idxs]\n",
    "        w = doctor_weights(len(pool), shift, is_weekend, week_parity)\n",
    "\n",
    "    # Continuity boost within (case, role, shift)\n",
    "    case_key = (row[case_id_col], role, shift)\n",
    "    prev = _last_resource_for.get(case_key)\n",
    "    if prev in pool:\n",
    "        j = pool.index(prev)\n",
    "        w[j] *= CONTINUITY_GAMMA\n",
    "        w /= w.sum()\n",
    "\n",
    "    rng = rng_for_row(row, extra_key=f\"{shift}|{is_weekend}|{week_parity}\")  # season removed\n",
    "    chosen = pick(pool, w, rng)\n",
    "\n",
    "    _last_resource_for[case_key] = chosen\n",
    "    return chosen\n",
    "\n",
    "# Apply to event log\n",
    "log_df[\"resource\"] = log_df.apply(assign_resource, axis=1)\n",
    "\n",
    "# Summary\n",
    "print(\"Resource distribution by activity (top 10 shown):\")\n",
    "summary = (log_df.groupby([\"activity\", \"resource\"]).size().sort_values(ascending=False))\n",
    "print(summary.head(10))\n",
    "display(log_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01cfd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Prefer recommended counts; only fall back if not available\n",
    "# n_nurses = int(globals().get('recommended_n_nurses', globals().get('n_nurses', 6)))\n",
    "# n_doctors = int(globals().get('recommended_n_doctors', globals().get('n_doctors', 3)))\n",
    "\n",
    "# # -----------------------------\n",
    "# # Kept knobs (no season needed)\n",
    "# # -----------------------------\n",
    "# RHO_TARGET = 0.80  # target utilization\n",
    "# SERVICE_RATE_PER_ROLE = {  # completions/hour per resource\n",
    "#     \"nurse\": 6.0,\n",
    "#     \"doctor\": 3.0,\n",
    "# }\n",
    "# TAU_BY_SHIFT = {\"day\": 1.2, \"evening\": 1.0, \"night\": 0.7, \"unknown\": 1.0}\n",
    "# CONTINUITY_GAMMA = 2.0\n",
    "# SHIFT_STEP = {\"day\": 3, \"evening\": 5, \"night\": 7, \"unknown\": 3}  # for full-roster rotation\n",
    "\n",
    "# # Helper: generate alphabetic suffixes: A..Z, AA..ZZ, AAA..\n",
    "# from typing import List\n",
    "\n",
    "# def _alpha_suffixes(n: int) -> List[str]:\n",
    "#     def _to_letters(idx: int) -> str:\n",
    "#         # Excel-like column naming: 0->A, 25->Z, 26->AA, etc.\n",
    "#         s = \"\"\n",
    "#         x = idx + 1\n",
    "#         while x > 0:\n",
    "#             x, rem = divmod(x - 1, 26)\n",
    "#             s = chr(ord('A') + rem) + s\n",
    "#         return s\n",
    "#     return [_to_letters(i) for i in range(max(0, int(n)))]\n",
    "\n",
    "# # Ensure names use only letters after the underscore, lowercase role prefix\n",
    "# nurse_names = [f\"nurse_{s}\" for s in _alpha_suffixes(n_nurses)]\n",
    "# doctor_names = [f\"doctor_{s}\" for s in _alpha_suffixes(n_doctors)]\n",
    "\n",
    "# # Helper: deterministic RNG per event\n",
    "\n",
    "# def rng_for_row(row, extra_key: str = \"\"):\n",
    "#     # Seed based on case, activity, date, and shift key so assignment is stable across runs\n",
    "#     date_str = row[\"timestamp\"].strftime(\"%Y-%m-%d\") if pd.notna(row[\"timestamp\"]) else \"NA\"\n",
    "#     key = f\"{row[case_id_col]}|{row['activity']}|{date_str}|{extra_key}\"\n",
    "#     seed = int(hashlib.sha1(key.encode(\"utf-8\")).hexdigest(), 16) % (2**32)\n",
    "#     return np.random.default_rng(seed)\n",
    "\n",
    "# # Shift + weekend + week parity (parity kept only for pivot rotation)\n",
    "# def shift_and_week(row):\n",
    "#     ts = row[\"timestamp\"]\n",
    "#     if pd.isna(ts):\n",
    "#         return \"unknown\", False, 0\n",
    "#     hour = ts.hour\n",
    "#     if 7 <= hour < 15:\n",
    "#         shift = \"day\"\n",
    "#     elif 15 <= hour < 23:\n",
    "#         shift = \"evening\"\n",
    "#     else:\n",
    "#         shift = \"night\"\n",
    "#     is_weekend = ts.dayofweek >= 5\n",
    "#     week_parity = int(ts.isocalendar().week) % 2\n",
    "#     return shift, is_weekend, week_parity\n",
    "\n",
    "# # Activity -> role mapping\n",
    "# if 'activity_role' not in globals():\n",
    "#     activity_role = {\n",
    "#         \"registration\": \"nurse\",\n",
    "#         \"arrivel\": \"nurse\",\n",
    "#         \"triage\": \"nurse\",\n",
    "#         \"start treatment\": \"doctor\",\n",
    "#         \"depart\": \"nurse\",\n",
    "#     }\n",
    "\n",
    "\n",
    "# def active_indices(n: int, fraction: float, parity: int) -> list[int]:\n",
    "#     if n <= 0:\n",
    "#         return []\n",
    "#     k = max(1, int(round(n * float(fraction))))\n",
    "#     start = (2 * parity) % n  # rotate weekly\n",
    "#     return [int((start + i) % n) for i in range(k)]\n",
    "\n",
    "# # Dynamic weight builders so patterns scale with pool size\n",
    "\n",
    "# def nurse_weights(n: int, shift: str, is_weekend: bool, week_parity: int) -> np.ndarray:\n",
    "#     if n <= 0:\n",
    "#         return np.array([])\n",
    "#     # Choose a pivot index for the most-likely nurse depending on shift/weekend and parity\n",
    "#     if shift == \"night\":\n",
    "#         pivot = (2 + week_parity) % n if n >= 3 else (week_parity % n)\n",
    "#     elif is_weekend:\n",
    "#         pivot = (1 + week_parity) % n if n >= 2 else 0\n",
    "#     else:\n",
    "#         pivot = (0 + week_parity) % n if n >= 2 else 0\n",
    "#     w = np.full(n, 0.1)\n",
    "#     w[pivot] += 0.4  # primary\n",
    "#     if n >= 2:\n",
    "#         w[(pivot+1) % n] += 0.2  # secondary\n",
    "#     if n >= 3:\n",
    "#         w[(pivot+2) % n] += 0.1  # tertiary\n",
    "#     w = w / w.sum()\n",
    "#     return w\n",
    "\n",
    "\n",
    "# def doctor_weights(n: int, shift: str, is_weekend: bool, week_parity: int) -> np.ndarray:\n",
    "#     if n <= 0:\n",
    "#         return np.array([])\n",
    "#     # Favor second doctor on nights/weekends, rotate weekly if possible\n",
    "#     if n >= 2:\n",
    "#         pivot = 1 if (is_weekend or shift == \"night\") else 0\n",
    "#         pivot = (pivot + week_parity) % n\n",
    "#     else:\n",
    "#         pivot = 0\n",
    "#     w = np.full(n, 0.2)\n",
    "#     w[pivot] += 0.4  # primary\n",
    "#     if n >= 2:\n",
    "#         w[(pivot+1) % n] += 0.2  # secondary\n",
    "#     w = w / w.sum()\n",
    "#     return w\n",
    "\n",
    "# # Weighted choice wrapper\n",
    "\n",
    "# def pick(pool, weights, rng):\n",
    "#     idx = rng.choice(len(pool), p=weights)\n",
    "#     return pool[int(idx)]\n",
    "\n",
    "# # Activity -> role mapping (respect pre-defined mapping if present)\n",
    "# if 'activity_role' not in globals():\n",
    "#     activity_role = {\n",
    "#         \"registration\": \"nurse\",\n",
    "#         \"arrivel\": \"nurse\",\n",
    "#         \"triage\": \"nurse\",\n",
    "#         \"start treatment\": \"doctor\",\n",
    "#         \"depart\": \"nurse\",\n",
    "#     }\n",
    "\n",
    "# # Assignment logic per row\n",
    "\n",
    "# def assign_resource(row):\n",
    "#     role = activity_role.get(row[\"activity\"], \"nurse\")\n",
    "#     shift, is_weekend, week_parity = shift_and_week(row)\n",
    "#     ts = row[\"timestamp\"]\n",
    "#     season = season_of(ts)\n",
    "\n",
    "#     # Determine active roster based on season/shift/weekend\n",
    "#     if role == \"nurse\":\n",
    "#         frac = nurse_active_fraction.get(season, nurse_active_fraction[\"spring\"]).get(shift, 0.9)\n",
    "#         if is_weekend:\n",
    "#             frac = max(frac, 0.95)\n",
    "#         indices = active_indices(len(nurse_names), frac, week_parity)\n",
    "#         pool = [nurse_names[i] for i in indices]\n",
    "#         w = nurse_weights(len(pool), shift, is_weekend, week_parity)\n",
    "#     else:\n",
    "#         frac = doctor_active_fraction.get(season, doctor_active_fraction[\"spring\"]).get(shift, 0.9)\n",
    "#         if is_weekend and shift in (\"evening\", \"night\"):\n",
    "#             frac = max(frac, 1.0)\n",
    "#         indices = active_indices(len(doctor_names), frac, week_parity)\n",
    "#         pool = [doctor_names[i] for i in indices]\n",
    "#         w = doctor_weights(len(pool), shift, is_weekend, week_parity)\n",
    "\n",
    "#     rng = rng_for_row(row, extra_key=f\"{shift}|{is_weekend}|{week_parity}|{season}\")\n",
    "#     return pick(pool, w, rng)\n",
    "\n",
    "# # Apply to event log\n",
    "# log_df[\"resource\"] = log_df.apply(assign_resource, axis=1)\n",
    "\n",
    "# # Concise distribution summary\n",
    "# print(\"Resource distribution by activity (top 10 shown):\")\n",
    "# summary = (log_df.groupby([\"activity\", \"resource\"])  # type: ignore\n",
    "#                     .size()\n",
    "#                     .sort_values(ascending=False))\n",
    "# print(summary.head(10))\n",
    "\n",
    "# display(log_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e18d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from math import exp\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Configuration knobs (feel free to move these near other global config cells)\n",
    "# TIME_PREF_CENTERS_FRAC = [0.10, 0.45, 0.80]   # fractional positions of time range for resource preference peaks\n",
    "# TIME_PREF_MIN_SPREAD_MINUTES = 5.0            # lower bound on Gaussian sigma (minutes)\n",
    "# MAX_RESOURCES_PER_ROLE = 50                   # safety cap if role list is huge\n",
    "# CONTINUITY_GAMMA_TIME_DEP = 2  # reuse existing continuity strength\n",
    "# RNG_SEED_TIME_DEP = 123\n",
    "\n",
    "# rng_time_dep = np.random.default_rng(RNG_SEED_TIME_DEP)\n",
    "\n",
    "# # Precompute relative time per event if not already present\n",
    "# _rel_col = 'relative_timestamp_from_start'\n",
    "# if _rel_col not in log_df.columns:\n",
    "#     # Use first timestamp per case\n",
    "#     _first_ts = log_df.groupby(case_id_col)['timestamp'].transform('min')\n",
    "#     log_df[_rel_col] = (log_df['timestamp'] - _first_ts).dt.total_seconds() / 60.0\n",
    "# else:\n",
    "#     log_df[_rel_col] = pd.to_numeric(log_df[_rel_col], errors='coerce').fillna(0.0)\n",
    "\n",
    "# # Establish max (e.g. 95th percentile) to define range for centers\n",
    "# _time_range_95 = float(log_df[_rel_col].quantile(0.95)) if len(log_df) else 60.0\n",
    "# _time_range_95 = max(_time_range_95, 60.0)  # at least 1 hour to avoid degenerate ranges\n",
    "\n",
    "# _centers_minutes = [c * _time_range_95 for c in TIME_PREF_CENTERS_FRAC]\n",
    "# _sigma_minutes = max(TIME_PREF_MIN_SPREAD_MINUTES, 0.25 * (_centers_minutes[-1] - _centers_minutes[0]))\n",
    "\n",
    "# # Build per-role resource lists from existing variables\n",
    "# nurse_list  = nurse_names[:MAX_RESOURCES_PER_ROLE]\n",
    "# doctor_list = doctor_names[:MAX_RESOURCES_PER_ROLE]\n",
    "\n",
    "# # Assign each resource a center (cycle through centers if more resources than centers)\n",
    "# _resource_centers = {}\n",
    "# for role_list in [nurse_list, doctor_list]:\n",
    "#     for i, r in enumerate(role_list):\n",
    "#         _resource_centers[r] = _centers_minutes[i % len(_centers_minutes)]\n",
    "\n",
    "# # Helper: compute unnormalized time weight for a resource at elapsed minutes t\n",
    "# _DEF_BASE_EPS = 1e-6\n",
    "\n",
    "# def _time_weight(resource: str, t_minutes: float) -> float:\n",
    "#     c = _resource_centers.get(resource)\n",
    "#     if c is None:\n",
    "#         # fallback flat weight for resources not in center mapping\n",
    "#         return 1.0\n",
    "#     diff = t_minutes - c\n",
    "#     return exp(-0.5 * (diff / (_sigma_minutes + 1e-9)) ** 2) + _DEF_BASE_EPS\n",
    "\n",
    "# # Maintain per-case previous resource for continuity\n",
    "# _prev_resource_by_case = {}\n",
    "\n",
    "# # Optionally cache per-activity role (already in activity_role)\n",
    "# # activity_role: dict mapping Activity -> role string (e.g. 'nurse' or 'doctor')\n",
    "\n",
    "# # Function: drop-in replacement for previous assign_resource\n",
    "\n",
    "# def assign_resource_time_dependent(row: pd.Series) -> str:\n",
    "#     \"\"\"Time-dependent resource assignment creating R–T dependence within activities.\n",
    "#     Uses a Gaussian time preference per resource and a continuity boost.\n",
    "\n",
    "#     Assumptions:\n",
    "#       - `row` contains at least: case_id_col, 'Activity', 'time:timestamp'.\n",
    "#       - Global lists nurse_names, doctor_names define candidate resources per role.\n",
    "#       - activity_role maps activity name to either 'nurse' or 'doctor'; default fallback to nurse list if unknown.\n",
    "#     \"\"\"\n",
    "#     case_id = row[case_id_col]\n",
    "#     activity = row['Activity'] if 'Activity' in row else row.get('concept:name')\n",
    "#     t_minutes = float(row[_rel_col]) if _rel_col in row else 0.0\n",
    "\n",
    "#     role = activity_role.get(activity, 'nurse').lower()\n",
    "#     if role.startswith('nurse'):\n",
    "#         candidates = nurse_list\n",
    "#     elif role.startswith('doctor'):\n",
    "#         candidates = doctor_list\n",
    "#     else:\n",
    "#         # fallback merges both pools to avoid empty candidate set\n",
    "#         candidates = nurse_list + doctor_list\n",
    "#         if not candidates:\n",
    "#             return 'UNKNOWN'\n",
    "\n",
    "#     if not candidates:\n",
    "#         return 'NO_RESOURCE'\n",
    "\n",
    "#     # Build time-conditioned weights\n",
    "#     weights = np.array([_time_weight(r, t_minutes) for r in candidates], dtype=float)\n",
    "\n",
    "#     # Continuity boost if previous resource same case\n",
    "#     prev_r = _prev_resource_by_case.get(case_id)\n",
    "#     if prev_r in candidates and CONTINUITY_GAMMA_TIME_DEP > 1.0:\n",
    "#         j = candidates.index(prev_r)\n",
    "#         weights[j] *= CONTINUITY_GAMMA_TIME_DEP\n",
    "\n",
    "#     # Normalize and sample\n",
    "#     total = weights.sum()\n",
    "#     if total <= 0 or not np.isfinite(total):\n",
    "#         weights = np.ones(len(candidates), dtype=float)\n",
    "#         total = weights.sum()\n",
    "#     probs = weights / total\n",
    "#     chosen = rng_time_dep.choice(candidates, p=probs)\n",
    "\n",
    "#     # Update continuity state\n",
    "#     _prev_resource_by_case[case_id] = chosen\n",
    "#     return chosen\n",
    "\n",
    "# # Apply the new function to assign resources\n",
    "# log_df['Resource'] = log_df.apply(assign_resource_time_dependent, axis=1)\n",
    "# print(\"[assign_resource_time_dependent] Assigned resources with time-conditioned + continuity logic.\")\n",
    "# display(log_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6e6f0",
   "metadata": {},
   "source": [
    "### Active roster preview\n",
    "Use this cell to preview who is on-duty for a given timestamp, per role (nurse/doctor).\n",
    "- It shows: season, shift, weekend flag, week parity, the active fraction, the rotating start index, and the active names.\n",
    "- It also shows assignment weights within the active pool (higher weight = more likely to be assigned).\n",
    "- The result is deterministic for the same timestamp and data (stable seeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preview parameters\n",
    "# PREVIEW_TS = None  # Set to e.g., '2012-01-10 10:30'; if None, uses the first timestamp in the log\n",
    "# MAX_SHOW = 10      # How many top-weighted names to show per role\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def _pick_preview_timestamp():\n",
    "#     if PREVIEW_TS is not None:\n",
    "#         return pd.to_datetime(PREVIEW_TS)\n",
    "#     if 'log_df' in globals() and 'timestamp' in log_df.columns:\n",
    "#         ts_series = pd.to_datetime(log_df['timestamp'], errors='coerce').dropna().sort_values()\n",
    "#         if not ts_series.empty:\n",
    "#             return ts_series.iloc[0]\n",
    "#     # Fallback: now (note: season/shift logic still works)\n",
    "#     return pd.Timestamp.now()\n",
    "\n",
    "# def _active_roster_for_role(role, ts, shift, is_weekend, week_parity, season):\n",
    "#     if role not in ('nurse', 'doctor'):\n",
    "#         raise ValueError('role must be nurse or doctor')\n",
    "#     names = nurse_names if role == 'nurse' else doctor_names\n",
    "#     n = len(names)\n",
    "#     if n == 0:\n",
    "#         return {\n",
    "#             'fraction': 0.0, 'n': 0, 'k': 0, 'start': 0, 'indices': [], 'pool': [], 'weights': np.array([])\n",
    "#         }\n",
    "\n",
    "#     if role == 'nurse':\n",
    "#         frac = nurse_active_fraction.get(season, nurse_active_fraction['spring']).get(shift, 0.9)\n",
    "#         if is_weekend:\n",
    "#             frac = max(frac, 0.95)\n",
    "#         k = max(1, int(round(n * float(frac))))\n",
    "#         start = (2 * int(week_parity)) % n\n",
    "#         indices = [int((start + i) % n) for i in range(k)]\n",
    "#         pool = [names[i] for i in indices]\n",
    "#         w = nurse_weights(len(pool), shift, is_weekend, week_parity) if len(pool) > 0 else np.array([])\n",
    "#     else:  # doctor\n",
    "#         frac = doctor_active_fraction.get(season, doctor_active_fraction['spring']).get(shift, 0.9)\n",
    "#         if is_weekend and shift in ('evening', 'night'):\n",
    "#             frac = max(frac, 1.0)\n",
    "#         k = max(1, int(round(n * float(frac))))\n",
    "#         start = (2 * int(week_parity)) % n\n",
    "#         indices = [int((start + i) % n) for i in range(k)]\n",
    "#         pool = [names[i] for i in indices]\n",
    "#         w = doctor_weights(len(pool), shift, is_weekend, week_parity) if len(pool) > 0 else np.array([])\n",
    "\n",
    "#     return {'fraction': float(frac), 'n': n, 'k': k, 'start': start, 'indices': indices, 'pool': pool, 'weights': w}\n",
    "\n",
    "# # Determine context for preview\n",
    "# ts = _pick_preview_timestamp()\n",
    "# row = pd.Series({'timestamp': ts})\n",
    "# shift, is_weekend, week_parity = shift_and_week(row)\n",
    "# season = season_of(ts)\n",
    "\n",
    "# print(f\"Previewing active roster at {ts} → shift={shift}, weekend={is_weekend}, parity={week_parity}, season={season}\")\n",
    "\n",
    "# for role in ('nurse', 'doctor'):\n",
    "#     info = _active_roster_for_role(role, ts, shift, is_weekend, week_parity, season)\n",
    "#     print(f\"\\nRole: {role}\")\n",
    "#     print(f\"- total names (n): {info['n']}\")\n",
    "#     print(f\"- active fraction (f): {info['fraction']:.2f}\")\n",
    "#     print(f\"- active count (k): {info['k']}\")\n",
    "#     print(f\"- rotating start index: {info['start']}\")\n",
    "#     print(f\"- active names: {len(info['pool'])}\")\n",
    "#     if info['pool'] and len(info['weights']) == len(info['pool']):\n",
    "#         dfw = pd.DataFrame({'name': info['pool'], 'weight': info['weights']})\n",
    "#         dfw['weight_pct'] = (dfw['weight'] / dfw['weight'].sum() * 100).round(2) if dfw['weight'].sum() else 0\n",
    "#         display(dfw.sort_values('weight', ascending=False).head(int(MAX_SHOW)))\n",
    "#     else:\n",
    "#         print('(no active names to show)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a5247",
   "metadata": {},
   "source": [
    "### Add relative timestamp between activities\n",
    "Compute the per-event delay since the previous event in the same case using the utility function, and attach it as `relative_timestamp_from_previous_activity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca14579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure canonical timestamp column name\n",
    "# Prefer 'time:timestamp' across the pipeline; rename if an older 'timestamp' exists\n",
    "if 'time:timestamp' not in log_df.columns and 'timestamp' in log_df.columns:\n",
    "    log_df = log_df.rename(columns={'timestamp': 'time:timestamp'})\n",
    "    log_df = log_df.rename(columns={'activity': 'Activity'})\n",
    "    log_df = log_df.rename(columns={'resource': 'Resource'})\n",
    "\n",
    "# Parse and sort to satisfy downstream utilities\n",
    "log_df['time:timestamp'] = pd.to_datetime(log_df['time:timestamp'], errors='coerce')\n",
    "\n",
    "log_df = log_df.sort_values([case_id_col, 'time:timestamp']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61eac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add relative timestamp between activities and from log start using the project utilities\n",
    "from src.preprocess_log import add_relative_timestamp_between_activities, add_trace_attr_relative_timestamp_to_first_activity\n",
    "\n",
    "# At this point, 'time:timestamp' should be the canonical timestamp column (see prior cell)\n",
    "# Ensure correct dtype and sort order\n",
    "log_df['time:timestamp'] = pd.to_datetime(log_df['time:timestamp'], errors='coerce')\n",
    "log_df = log_df.sort_values([case_id_col, 'time:timestamp']).reset_index(drop=True)\n",
    "\n",
    "# 1) Minutes since previous event in the same case\n",
    "log_df = add_relative_timestamp_between_activities(\n",
    "    log_df,\n",
    "    trace_key=case_id_col,\n",
    "    timestamp_key='time:timestamp',\n",
    "    custom_timestamp_key='relative_timestamp_from_previous_activity',\n",
    ")\n",
    "\n",
    "log_df = add_trace_attr_relative_timestamp_to_first_activity(\n",
    "    log_df,\n",
    "    trace_key=case_id_col,\n",
    "    timestamp_key='time:timestamp',\n",
    "    custom_timestamp_key='relative_timestamp_from_start',\n",
    ")\n",
    "\n",
    "log_df[\"label\"] = log_df['Gender']\n",
    "\n",
    "print(\"Added column: 'relative_timestamp_from_previous_activity' (minutes)\")\n",
    "\n",
    "display(log_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-event relative time from case start (minutes)\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "# Expect these to already exist from earlier cells:\n",
    "# - df: events DataFrame\n",
    "#! - case_id_col: name of the case id column (e.g., 'case:concept:name')\n",
    "#! - ts_col: name of the timestamp column (e.g., 'time:timestamp')\n",
    "\n",
    "assert 'df' in globals(), \"DataFrame 'df' not found in notebook scope\"\n",
    "assert 'case_id_col' in globals(), \"Variable 'case_id_col' not found\"\n",
    "\n",
    "# Ensure timestamp is datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(log_df[\"time:timestamp\"]):\n",
    "    log_df[\"time:timestamp\"] = pd.to_datetime(log_df[\"time:timestamp\"], errors='coerce')\n",
    "\n",
    "# Sort within case then compute offset from first event per case\n",
    "log_df = log_df.sort_values([case_id_col, \"time:timestamp\"])\n",
    "log_df['relative_time'] = (\n",
    "    log_df.groupby(case_id_col)[\"time:timestamp\"]\n",
    "      .transform(lambda s: (s - s.min()).dt.total_seconds() / 60.0)\n",
    ")\n",
    "\n",
    "print(\"[rel-time] Created 'relative_time' (head):\")\n",
    "print(log_df[[case_id_col, \"time:timestamp\", 'relative_time']].head())\n",
    "display(log_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfc821",
   "metadata": {},
   "source": [
    "## Save event log to CSV\n",
    "\n",
    "This saves the current `log_df` to `output/event_log.csv` under the repository root.\n",
    "- Columns are ordered with `[case_id, activity, timestamp]` first for convenience.\n",
    "- The folder is created if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00758bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save event log\n",
    "SAVE_DIR = base_root / 'data' / 'emergency_ORT'\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAVE_PATH = SAVE_DIR / 'emergency_ORT.csv'\n",
    "\n",
    "# Rename case id column to 'Case ID' for output\n",
    "out_df = log_df.copy()\n",
    "if case_id_col in out_df.columns and case_id_col != 'Case ID':\n",
    "    out_df.rename(columns={case_id_col: 'Case ID'}, inplace=True)\n",
    "\n",
    "# Reorder columns: Case ID, activity, timestamp first\n",
    "cols_front = ['Case ID', 'activity', 'timestamp']\n",
    "cols_front = [c for c in cols_front if c in out_df.columns]\n",
    "other_cols = [c for c in out_df.columns if c not in cols_front]\n",
    "out_df = out_df.loc[:, cols_front + other_cols]\n",
    "\n",
    "# Save with semicolon delimiter\n",
    "out_df.to_csv(SAVE_PATH, index=False, sep=';')\n",
    "print(f\"Saved event log to: {SAVE_PATH}\")\n",
    "print(f\"Rows: {len(out_df):,}, Columns: {len(out_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0963062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d10773f",
   "metadata": {},
   "source": [
    "### Split the saved event log into TRAIN/VAL/TEST (temporal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the saved event log\n",
    "from src.split_log import split_temporal\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure we have a saved CSV path from the previous cell\n",
    "if 'SAVE_PATH' not in globals():\n",
    "    raise RuntimeError(\"SAVE_PATH not found — run the 'Save event log' cell first.\")\n",
    "\n",
    "# Derive output directory and base name\n",
    "_csv_path = Path(SAVE_PATH)\n",
    "_out_dir = _csv_path.parent  # same folder as saved log\n",
    "_log_name = _csv_path.stem\n",
    "\n",
    "# Workaround: create a temp CSV with a 'time:timestamp' alias so read_log parses datetimes\n",
    "_tmp_path = _csv_path.with_name(f\"{_log_name}__split_tmp.csv\")\n",
    "_df_tmp = pd.read_csv(_csv_path, sep=';')\n",
    "if 'time:timestamp' not in _df_tmp.columns and 'timestamp' in _df_tmp.columns:\n",
    "    _df_tmp['time:timestamp'] = _df_tmp['timestamp']\n",
    "_df_tmp.to_csv(_tmp_path, index=False, sep=';')\n",
    "\n",
    "# Split 60/20/20 by case start time, using our column names\n",
    "split_temporal(\n",
    "    log_path=str(_tmp_path),\n",
    "    log_name=_log_name,\n",
    "    output_path=str(_out_dir),\n",
    "    split_perc=[0.6, 0.2, 0.2],\n",
    "    csv_sep=';',\n",
    "    case_id_key='Case ID',\n",
    "    timestamp_key='time:timestamp',\n",
    ")\n",
    "\n",
    "print(f\"Wrote TRAIN/VAL/TEST splits to: {_out_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b2cbd",
   "metadata": {},
   "source": [
    "## Quick cHSIC check on current log\n",
    "\n",
    "This cell computes cHSIC(R, T | A) on the in-memory `log_df` using the same approach as the evaluation pipeline:\n",
    "- For each activity, build a categorical kernel on Resource and a multi-scale RBF on relative time (minutes since case start).\n",
    "- Use the unbiased HSIC estimator when feasible, else biased.\n",
    "- Report per-activity HSICs and uniform/frequency-weighted aggregates.\n",
    "\n",
    "Note: If you changed resource assignment just above, re-run that cell first, then run this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(log_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cHSIC(R, T | A) — in-notebook check aligned with evaluation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1) Prepare DF with standardized schema\n",
    "_df = log_df.copy()\n",
    "\n",
    "# Detect schema and unify column names\n",
    "if {'case:concept:name', 'concept:name', 'org:resource'}.issubset(_df.columns):\n",
    "    case_col, act_col, res_col = 'case:concept:name', 'concept:name', 'org:resource'\n",
    "elif {'SEHRegistratienummer', 'Activity', 'Resource'}.issubset(_df.columns):\n",
    "    _df = _df.rename(columns={'SEHRegistratienummer': 'case:concept:name','Activity': 'concept:name','Resource': 'org:resource'})\n",
    "    case_col, act_col, res_col = 'case:concept:name', 'concept:name', 'org:resource'\n",
    "else:\n",
    "    raise ValueError(\"Missing required columns: expected either ['case:concept:name','concept:name','org:resource'] or ['Case ID','Activity','Resource']\")\n",
    "\n",
    "# If a clean 'Resource' column exists, prefer it to override any pre-existing 'org:resource'\n",
    "if 'Resource' in _df.columns and _df['Resource'].notna().any():\n",
    "    _df['org:resource'] = _df['Resource'].astype(str)\n",
    "\n",
    "# Robustly coerce org:resource to scalar strings (guard against list/tuple/ndarray entries)\n",
    "# Keep a helper that can stringify nested sequences deterministically\n",
    "\n",
    "def _res_to_str(x):\n",
    "    if isinstance(x, (list, tuple, np.ndarray)):\n",
    "        try:\n",
    "            return '|'.join(map(str, list(x)))\n",
    "        except Exception:\n",
    "            return str(x)\n",
    "    return str(x)\n",
    "\n",
    "# Apply once globally to catch typical cases\n",
    "_df['org:resource'] = _df['org:resource'].apply(_res_to_str)\n",
    "\n",
    "if 'time:timestamp' not in _df.columns:\n",
    "    raise ValueError(\"Missing 'time:timestamp' column for cHSIC prep.\")\n",
    "\n",
    "# Ensure dtypes\n",
    "_df['concept:name'] = _df['concept:name'].astype(str)\n",
    "_df['org:resource'] = _df['org:resource'].astype(str)\n",
    "_df['time:timestamp'] = pd.to_datetime(_df['time:timestamp'])\n",
    "\n",
    "# 2) HSIC utilities — mirroring evaluation pipeline\n",
    "\n",
    "def _rbf_multi(x: torch.Tensor, y: torch.Tensor, sigmas: torch.Tensor) -> torch.Tensor:\n",
    "    d2 = torch.cdist(x, y, p=2).pow(2)\n",
    "    K = 0.0\n",
    "    for s in sigmas:\n",
    "        K = K + torch.exp(-d2 / (2.0 * (s * s + 1e-12)))\n",
    "    return K\n",
    "\n",
    "def _delta_eq(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    return (x[:, None] == y[None, :]).float()\n",
    "\n",
    "def _hsic_unbiased(K: torch.Tensor, L: torch.Tensor) -> float:\n",
    "    n = K.shape[0]\n",
    "    if n < 4:\n",
    "        return _hsic_biased(K, L)\n",
    "    K2 = K.clone(); L2 = L.clone()\n",
    "    K2.fill_diagonal_(0.0); L2.fill_diagonal_(0.0)\n",
    "    H = torch.eye(n, device=K.device) - (1.0 / n) * torch.ones((n, n), device=K.device)\n",
    "    KH = K2 @ H\n",
    "    LH = L2 @ H\n",
    "    val = (KH * LH).sum() / (n * (n - 3))\n",
    "    return float(val.item())\n",
    "\n",
    "def _hsic_biased(K: torch.Tensor, L: torch.Tensor) -> float:\n",
    "    n = K.shape[0]\n",
    "    H = torch.eye(n, device=K.device) - (1.0 / n) * torch.ones((n, n), device=K.device)\n",
    "    Kc = H @ K @ H\n",
    "    Lc = H @ L @ H\n",
    "    val = (Kc * Lc).sum() / (n * n)\n",
    "    return float(val.item())\n",
    "\n",
    "def _median_bandwidth(v: torch.Tensor) -> float:\n",
    "    n = v.shape[0]\n",
    "    if n < 2:\n",
    "        return 1.0\n",
    "    exact_limit = 4000\n",
    "    sample_size = 1024\n",
    "    mad_fallback_limit = 20000\n",
    "    if n <= exact_limit:\n",
    "        d = torch.cdist(v, v, p=2)\n",
    "        mask = d > 0.0\n",
    "        med = torch.median(d[mask]) if torch.any(mask) else torch.tensor(1.0, device=v.device)\n",
    "        return float(max(med.item(), 1e-3))\n",
    "    if n > mad_fallback_limit:\n",
    "        v_flat = v.view(-1)\n",
    "        median_v = v_flat.median()\n",
    "        mad = (v_flat - median_v).abs().median()\n",
    "        approx = float((mad * 1.41421356237).item())\n",
    "        return max(approx, 1e-3)\n",
    "    with torch.no_grad():\n",
    "        idx = torch.randperm(n, device=v.device)[:sample_size]\n",
    "        vs = v[idx]\n",
    "        d_sample = torch.cdist(vs, vs, p=2)\n",
    "        mask = d_sample > 0.0\n",
    "        med = torch.median(d_sample[mask]) if torch.any(mask) else torch.tensor(1.0, device=v.device)\n",
    "    return float(max(med.item(), 1e-3))\n",
    "\n",
    "# 3) Compute per-activity HSIC\n",
    "_device = torch.device('cpu')\n",
    "profile = {}\n",
    "counts = _df['concept:name'].value_counts().to_dict()\n",
    "max_points = 4000\n",
    "\n",
    "for a, grp in _df.groupby('concept:name'):\n",
    "    # Build a clean 1D string series for resources — defensive against 2D/nested entries\n",
    "    vals = grp['org:resource'].to_numpy()\n",
    "    if isinstance(vals, np.ndarray) and vals.ndim > 1:\n",
    "        # Coerce each row (e.g., array([x,y])) to a single string 'x|y'\n",
    "        g_res = pd.Series(['|'.join(map(str, list(v))) for v in vals], index=grp.index)\n",
    "    else:\n",
    "        g_res = grp['org:resource'].apply(_res_to_str)\n",
    "\n",
    "    # Encode resource\n",
    "    r_codes, _ = pd.factorize(g_res, sort=True)\n",
    "    r = torch.from_numpy(np.asarray(r_codes, dtype=np.int64))\n",
    "\n",
    "    # Time numeric\n",
    "    t = torch.from_numpy(grp['relative_time'].values.astype(np.float32)).view(-1, 1)\n",
    "    n = t.shape[0]\n",
    "    if n < 3:\n",
    "        continue\n",
    "    # Subsample for memory safety\n",
    "    if n > max_points:\n",
    "        idx = torch.randperm(n)[:max_points]\n",
    "        t = t[idx]; r = r[idx]\n",
    "        n = max_points\n",
    "    # Kernels\n",
    "    s = _median_bandwidth(t)\n",
    "    sigmas = torch.tensor([s, 2*s, 4*s], dtype=torch.float32)\n",
    "    Kt = _rbf_multi(t, t, sigmas)\n",
    "    Kr = _delta_eq(r, r)\n",
    "    # HSIC\n",
    "    hsic_val = _hsic_unbiased(Kt, Kr) if n >= 4 else _hsic_biased(Kt, Kr)\n",
    "    profile[str(a)] = hsic_val\n",
    "\n",
    "# 4) Aggregates\n",
    "if profile:\n",
    "    uniform = float(np.mean(list(profile.values())))\n",
    "    total = 0.0; acc = 0.0\n",
    "    for a, v in profile.items():\n",
    "        w = counts.get(a, 0)\n",
    "        acc += w * v\n",
    "        total += w\n",
    "    freq_weighted = float(acc / total) if total > 0 else None\n",
    "else:\n",
    "    uniform = None; freq_weighted = None\n",
    "\n",
    "print(\"cHSIC per activity (first 10):\")\n",
    "for k, v in list(sorted(profile.items(), key=lambda kv: kv[1], reverse=True))[:10]:\n",
    "    print(f\"  {k}: {v:.6f}\")\n",
    "print(f\"\\nAggregate cHSIC — uniform: {uniform}, freq-weighted: {freq_weighted}\")\n",
    "\n",
    "# Optional: keep a small dict to inspect\n",
    "chsic_result = {\n",
    "    'per_activity': profile,\n",
    "    'aggregate': {'uniform': uniform, 'freq_weighted': freq_weighted},\n",
    "    'meta': {'num_events': int(len(_df))}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
